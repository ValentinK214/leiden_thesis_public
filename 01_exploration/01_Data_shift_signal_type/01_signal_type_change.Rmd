---
title: "Signal type change"
author: "Valentin Kodderitzsch"
date: "2025-04-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Global set up


The goal of this experiment was to investigate the impact of **signal type** changes in the German dataset on **rank instability**, using the Dutch dataset as the fixed reference.  

The reference signal type is level 2 (Dutch data), and German signal types vary across levels 1–4.  

The experiment follows a **nested Monte Carlo design**:  
- **Outer loop** (`B_global = 50`): Repeat the full experiment 50 times to account for **sampling variance**.  
- **Inner loop** (`n_signal_global = 4`): Within each repetition, vary the German dataset’s signal type, while keeping all other parameters fixed.  

```{r}
set.seed(3895157)

n_global = 10^3 * (4) # sample size
noise_global_NL = 0.1 # noise proportion
noise_global_DE = 0.1 # noise proportion DE

signal_type_global_NL = 2
signal_type_global_DE = 4

n_signal_global = 4

B_global = 50
```


# #################

# 2. Helper Functions

## Generate Data

True signal

```{r}
true_signal = function(x1, x2, x3, x4, type) {
  if (type > 4 ) {
    stop("Type > 4 not possible since only 4 true signals present")
  } else {
      switch(type,
         # Case 1
         x1 + x2 + x3 +  0.5*x4+1, # 0.5
         # Case 2
         x1 + x2 + x3 +  0.15*x4^2,
         # Case 3
         x1 + x2 + x3 +  exp(0.3*x4) * (1 + 0.6 * sin((2*pi/10) * x4)) ,
         # Case 4
         x1 + x2 + x3 +  exp(0.3*x4) * (1 + 0.9 * sin((2*pi/3) * x4)) # 3 instead of 0.3
         ) 
  }
}

true_signal(1, 2, 3, 4, type = 3)
```


Noise proportion

```{r}
# Helper function to compute error variance
get_error_for_signal_prop = function(signal_prop, signal_sd) {
  ((1-signal_prop) / signal_prop ) * signal_sd
}

# True signal variance
f3_signal_var = 0.1002295
```


**FUNCTION EXPLANATION**: `generate_data`  

**Args:**  
1. **noise** (numeric): proportion of variance due to irreducible error.  
2. **n** (integer): number of observations.  
3. **use_original_signal_var** (logical): if `TRUE` use `f3_signal_var`, otherwise estimate by simulation.  
4. **type** (character): passed to `true_signal(...)` to select the signal form.  

**Returns:**  
- **X**: numeric matrix (n × 4) of predictors  
- **y**: numeric response vector (length n)  
- **signal_var**: numeric; estimated or provided signal variance  
- **irreducible_error**: numeric; standard deviation of the additive Gaussian noise  

**Main logic:**  
1. draw 4 predictors from their uniform ranges (x1..x4)  
2. If `use_original_signal_var = FALSE`:  
   - Simulate `B = 10^3` noiseless responses (`true_signal(...)`) to estimate signal variance.  
   - Otherwise, set `signal_var = f3_signal_var`.  
3. Compute error variance for the requested noise proportion, take square root to obtain **irreducible error** -> raw irreducible error based on `irreducible_error` input 
4. Form `y = true_signal(...) + Normal(0, sd)` 



```{r}
generate_data = function(noise = noise_global, 
                         n = n_global,
                         use_original_signal_var = T,
                         type) {
  # Explanatory variables
  x1 = runif(n, min = 0, max = 20 )
  x2 = runif(n, min = 4 * pi, max = 5.6 * pi )
  x3 = runif(n, min = 0, max = 1 )
  x4 = runif(n, min = 1, max = 11 )
  
  X = data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4)
  X = as.matrix(X)
  
  if (!use_original_signal_var) {
    # Compute true signal variance
    B = 10^3
    signal_list = numeric(B)
    
    
    # Compute signal variance = variance of Y when the irreducible error equals zero
    for (idx in 1:B) {
      y_boot = true_signal(x1, x2, x3, x4, type)
      signal_list[idx] = var(y_boot)
    }
    
    signal_var = mean(signal_list)
    #print(paste("Calculatied signal var instead of using the original -> ", signal_var))
  } else {
      signal_var = f3_signal_var
      #print(paste("Used original signal var -> ", signal_var))
    }
  
  # Compute error variance
  error_var = get_error_for_signal_prop((1-noise), signal_var ) # f3_signal_var
  #print(paste("error var -> ", error_var))

  # Save irreducible error as standard deviation
  irreducible_error = sqrt(error_var)

  # Response
  y = true_signal(x1, x2, x3, x4, type) + rnorm(n, mean = 0, sd = irreducible_error)
  
  # Return list
  list(X = X, y = y, signal_var = signal_var, irreducible_error = irreducible_error)
}

# Sanity check
df_Dutch = generate_data( use_original_signal_var = F, 
                         noise = noise_global_NL, type = signal_type_global_NL) 
df_German = generate_data(use_original_signal_var = F, 
                          noise = noise_global_DE, type = signal_type_global_DE)
```



### Plot x4

```{r}
i=4
body(true_signal)[[2]][[4]][[2]][[2+i]][[3]]
```

```{r}
label_lines = sapply(1:n_signal_global, function(i) {
  fn_string = body(true_signal)[[2]][[4]][[2]][[2+i]]
  paste0("Signal type ", i, ": ", deparse(fn_string), " with noise prop = ", noise_global_NL)
})

signal_caption = paste(label_lines, collapse = "\n")
```


```{r}
get_lm_nrmse = function(input_df) {
  # Reformat input dataframe
  tmp_df = as.data.frame(input_df$X)
  tmp_df$y = input_df$y
  
  # Fit a linear regression model
  lm_fit = lm(y ~ ., data = tmp_df)
  y_hat = predict(lm_fit)
  
  # Compute RMSE
  mse = mean( (tmp_df$y - y_hat)^2 )
  round(sqrt(mse) / sd(tmp_df$y) , 4) # diff(range(tmp_df$y))
}

# X4 component
get_y_for_x4 = function(x4, signal_type) {
  expr = body(true_signal)[[2]][[4]][[2]][[2+signal_type]][[3]]
  # print(expr)
  eval(expr, envir = list(x4 = x4))
}


true_signal_all = function(X, signal_type) {
    y = numeric(nrow(X))
    for (idx in 1:nrow(X)) {
      tmp_row = X[idx, ]
      y[idx] = true_signal(x1 = tmp_row[1], x2 = tmp_row[2], x3 = tmp_row[3], x4 = tmp_row[4], type = signal_type)
    }
    return(y)
}

# Generate sequence data for all covariates
generate_seq_X_data = function(X, n_points = 1000) {
  # Get ranges for each covariate
  x1_range = range(X[, 1])
  x2_range = range(X[, 2])
  x3_range = range(X[, 3])
  x4_range = range(X[, 4])
  
  # Create sequences for each covariate
  x1_vals = seq(x1_range[1], x1_range[2], length.out = n_points)
  x2_vals = seq(x2_range[1], x2_range[2], length.out = n_points)
  x3_vals = seq(x3_range[1], x3_range[2], length.out = n_points)
  x4_vals = seq(x4_range[1], x4_range[2], length.out = n_points)
  
  # Create dataframe with sequences
  X_seq = data.frame(
    x1 = x1_vals,
    x2 = x2_vals,
    x3 = x3_vals,
    x4 = x4_vals
  )
  return(X_seq)
}

# Set up side-by-side plotting
par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))

# Function to create plot for each dataset
plot_dataset = function(df, country, color, signal_type) {
  # Extract x4
  x4 = df$X[, 4]
  
  # Get x4 range and create evaluation points
  x4_range = range(x4)
  x4_vals = seq(x4_range[1], x4_range[2], length.out = 1000)
  
  # Generate sequence data for all covariates
  X_seq = generate_seq_X_data(df$X, n_points = 1000)
  
  # Compute Y without noise
  y_pred = true_signal_all(X_seq, signal_type)
  y_range = range(c(df$y, y_pred))
  
  # Compute linear NRMSE
  lm_nrmse = get_lm_nrmse(df)
  
  # Get LM plot
  lm_fit = lm(df$y ~ x4)
  
  # Predict over x4_vals
  newdata = data.frame(x4 = x4_vals)
  y_hat_lm = predict(lm_fit, newdata = newdata)
  
  # Create plot
  plot(df$X[, 4], df$y,
       xlab = "x4", ylab = "y",
       main = paste(country, "(", lm_nrmse  ," LM NRMSE)"), 
       pch = 16, col = color,
       ylim = y_range, cex = 0.5)
  
  # Add fitted line
  lines(X_seq$x4, y_pred, col = "green", lwd = 2) # x4_vals instead of df$X[, 4]
  
  # Add LM line
  # lines(x4_vals, y_hat_lm, col = "orange", lwd = 2)
}

# Create plots
plot_dataset(df_Dutch, "Dutch", "blue", signal_type_global_NL)
plot_dataset(df_German, "German", "red", signal_type_global_DE)


# Set up side-by-side plotting
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1), oma = c(6, 0, 0, 0))

# New code:
for (signal_type in 1:n_signal_global) {
  # Generate data
  df_tmp = generate_data(use_original_signal_var = F, 
                         noise = noise_global_NL, type = signal_type)
  plot_dataset(df_tmp, paste("Signal type", signal_type), "blue", signal_type = signal_type)
}

mtext(signal_caption, side = 1, outer = TRUE, line = 4, cex = 0.8)

# Reset plotting parameters
par(mfrow = c(1, 1))
```


```{r}
# More sanity checks
check_noise_prop = function(df) {
  (df$irreducible_error)^2 / (df$signal_var + (df$irreducible_error)^2 ) 
}

check_noise_prop(df_Dutch)
check_noise_prop(df_German)

var(df_Dutch$y)
var(df_German$y)
```


```{r}
# Plot to check if the distributions are visibly different
plot(density(df_Dutch$y), main = "Dutch")
plot(density(df_German$y), main = "German")

boxplot(df_Dutch$y, main = "Dutch")
boxplot(df_German$y, main = "German")


# Compute densities
d1 = density(df_Dutch$y)
d2 = density(df_German$y)

# Set axis limits based on both
x_range = range(d1$x, d2$x)
y_max = max(d1$y, d2$y)

# Plot with fixed xlim and ylim
plot(d1, main = "Density of y: Dutch vs German", col = "blue", lwd = 2,
     xlim = x_range, ylim = c(0, y_max))

# Overlay the second density
lines(d2, col = "red", lwd = 2)

# Add legend
legend("topright", legend = c("Dutch", "German"), col = c("blue", "red"), lwd = 2)


```

```{r}
plot(df_German$X[, 1], df_German$y)
plot(df_Dutch$X[, 1], df_Dutch$y)


plot(df_German$X[, 2], df_German$y)
plot(df_Dutch$X[, 2], df_Dutch$y)

plot(df_German$X[, 3], df_German$y)
plot(df_Dutch$X[, 3], df_Dutch$y)

plot(df_German$X[, 4], df_German$y)
plot(df_Dutch$X[, 4], df_Dutch$y)


```


Compute KL and Janson Shanon Divergence scores as reference.

```{r}
kl_divergence_kde = function(p_samples, q_samples, n_points = 1000) {
  # Determine range to cover both distributions
  min_val = min(min(p_samples), min(q_samples))
  max_val = max(max(p_samples), max(q_samples))
  eval_points = seq(min_val, max_val, length.out = n_points)
  
  # Estimate densities using KDE
  p_density = density(p_samples, from = min_val, to = max_val, n = n_points)
  q_density = density(q_samples, from = min_val, to = max_val, n = n_points)
  
  # Extract density values
  p_values = p_density$y
  q_values = q_density$y
  
  # Add small epsilon to avoid division by zero or log(0)
  p_values = p_values + 1e-10
  q_values = q_values + 1e-10
  
  # Normalize to ensure they sum to 1
  p_values = p_values / sum(p_values)
  q_values = q_values / sum(q_values)
  
  # Calculate KL divergence: sum(p * log(p/q))
  kl = sum(p_values * log(p_values / q_values))
  
  return(kl)
}

# Function to calculate Jensen-Shannon divergence using KDE
js_divergence_kde = function(p_samples, q_samples, n_points = 1000) {
  # Determine range to cover both distributions
  min_val = min(min(p_samples), min(q_samples))
  max_val = max(max(p_samples), max(q_samples))
  eval_points = seq(min_val, max_val, length.out = n_points)
  
  # Estimate densities using KDE
  p_density = density(p_samples, from = min_val, to = max_val, n = n_points)
  q_density = density(q_samples, from = min_val, to = max_val, n = n_points)
  
  # Extract density values
  p_values = p_density$y
  q_values = q_density$y
  
  # Add small epsilon to avoid division by zero or log(0)
  p_values = p_values + 1e-10
  q_values = q_values + 1e-10
  
  # Normalize to ensure they sum to 1
  p_values = p_values / sum(p_values)
  q_values = q_values / sum(q_values)
  
  # Calculate the mixture distribution M = (P+Q)/2
  m_values = (p_values + q_values) / 2
  
  # Calculate KL(P||M) and KL(Q||M)
  kl_p_m = sum(p_values * log(p_values / m_values))
  kl_q_m = sum(q_values * log(q_values / m_values))
  
  # JS divergence is the average of the two KL divergences
  js = (kl_p_m + kl_q_m) / 2
  
  return(js)
}

kl = kl_divergence_kde(df_Dutch$y, df_German$y)
js = js_divergence_kde(df_Dutch$y, df_German$y)
print(paste("KL divergence:", kl))
print(paste("JS divergence:", js))
```


KL scores: 
0: Identical
0.1-1: Mild divergence
>1 Divergence 
>5 Large divergence

JSD interpretation:
JSD = 0 -> Distributions are identical.
JSD close to 0 -> Very similar.
JSD ~ 0.5 -> Moderately different.
JSD close to 1 -> Very different.




## Train-Test Algorithms


### Metrics

I evaluated 9 normalized metrics. 6 are well known: MSE, NSE, RSE, RRMSE, NRMSE and MAPE. 3 I developed myself to include a null model: PR, Min-Max and NDRMSE.


```{r}
get_MSE = function(test_Y, test_Y_Hat) {
  mean( (test_Y - test_Y_Hat)^2 )
}

get_NSE = function(test_Y, test_Y_Hat) {
  numerator = sum((test_Y - test_Y_Hat)^2)
  denominator = sum((test_Y - mean(test_Y))^2)
  1 - (numerator / denominator)
}

get_RSE = function(test_Y, test_Y_Hat) {
  numerator = sum((test_Y - test_Y_Hat)^2)
  denominator = sum((test_Y - mean(test_Y))^2)
  numerator / denominator
}

get_RRMSE = function(test_Y, test_Y_Hat) {
  rmse = sqrt(mean((test_Y - test_Y_Hat)^2))
  rmse / mean(test_Y)
}

get_NRMSE = function(test_Y, test_Y_Hat) {
  rmse = sqrt(mean((test_Y - test_Y_Hat)^2))
  rmse / ( sd(test_Y) ) #max(test_Y) - min(test_Y)
}

get_MAPE = function(test_Y, test_Y_Hat) {
  if (any(test_Y == 0)) {
    warning("My warning from get_MAP: test_Y contains zeros")
  } else {
    mean(abs( (test_Y - test_Y_Hat)/test_Y )) * 100
  }
}

get_all_6_metrics = function(test_Y, test_Y_Hat) {
  list(
    MSE   = get_MSE(test_Y, test_Y_Hat),
    NSE   = get_NSE(test_Y, test_Y_Hat),
    RSE   = get_RSE(test_Y, test_Y_Hat),
    RRMSE = get_RRMSE(test_Y, test_Y_Hat),
    NRMSE = get_NRMSE(test_Y, test_Y_Hat),
    MAPE  = get_MAPE(test_Y, test_Y_Hat)
  )
}
```



Performance ratio.

```{r}
get_PR = function(MSE_null, MSE_model) {
  sqrt(MSE_null / MSE_model)
}
```


Min-max metric

```{r}
get_min_max = function(MSE_null, MSE_model) {
  # (MSE_null - MSE_model) / MSE_null
  1 - (MSE_model / MSE_null)
}
```

ND-RMSE

```{r}
get_NDRMSE = function(MSE_null, MSE_model, test_Y) {
  numerator =  sqrt(MSE_null) - sqrt(MSE_model) 
  denominator = max(test_Y) - min(test_Y)
  numerator / denominator
  
  # sqrt(MSE_null) / sqrt(MSE_model)
}
```



### Final train-test function

```{r}
# Load all libraries
library(glmnet)
library(randomForest)
library(xgboost)
library("e1071")
library(tidyr)
library(dplyr)
```


**FUNCTION EXPLANATION**: `generate_summary_table`  

**Args:**  
1. **X** (matrix/data frame): predictor variables.  
2. **y** (numeric vector): response variable.  
3. **training_percentage** (numeric): proportion of data used for training (default: 0.75).  
4. **signal_type** (character): passed to `true_signal(...)` to specify the signal type.  

**Returns:**  
- A data frame (7x10) summarizing model performance across 9 metrics (MSE, NSE, RSE, RRMSE, NRMSE, MAPE, PR, MinMax, NDRMSE) for 7 algorithms:  
  - True signal (oracle)  
  - Dummy/null model  
  - Elastic net  
  - Random forest  
  - Linear regression  
  - XGBoost  
  - SVM  

**Main logic:**  
1. Split data into training and test sets.  
2. Define a helper to compute the true signal for all rows in `X` -> Needed by the Oracle 
3. Fit the following models on the training set:  
   - True signal (oracle, no noise)  
   - Null model (mean prediction)  
   - Elastic net (alpha = 0.5)  
   - Random forest  
   - Linear regression  
   - XGBoost  
   - SVM (epsilon regression)  
4. Generate predictions on the test set for each model.  
5. Compute 6 base metrics via `get_all_6_metrics()` for each model.  
6. Calculate additional metrics:  
   - **PR** (performance ratio via `get_PR()`)  
   - **MinMax** (via `get_min_max()`)  
   - **NDRMSE** (via `get_NDRMSE()`)  
7. Combine all results into a summary table.  
8. Reshape data for easier comparison (`pivot_longer` to `pivot_wider`) and sort models by PR in descending order.  
9. Return the final summary data frame.  



```{r}
generate_summary_table = function(X, y, training_percentage = 0.75, signal_type) {
  # Generate train test split
  train_percentage = training_percentage
  train_size = floor(nrow(X) * train_percentage) 

  train = sample(1:nrow(X), size = train_size)
  test = which(!(1:nrow(X) %in% train))
  
  ##################################################
  
  # Model fitting
  
  ## True signal
  true_signal_all = function(X) {
    y = numeric(nrow(X))
    for (idx in 1:nrow(X)) {
      tmp_row = X[idx, ]
      y[idx] = true_signal(x1 = tmp_row[1], x2 = tmp_row[2], x3 = tmp_row[3], x4 = tmp_row[4], type = signal_type)
    }
    return(y)
  }
  
  ## Null model
  null_model = y[train]
  
  ## Elatic net
  # Set alpha = 0.5 for balanced elastic net
  cv_elastic = cv.glmnet(
    x = X[train, ],
    y = y[train],
    alpha = 0.5,
    family = "gaussian",
    nfolds = 5
  )
  
  
  # Fit on entire dataset using optimal lambda value
  model_elastic = glmnet(x = X[train, ],
                         y = y[train],
                         lambda = cv_elastic$lambda.1se)
  
  ## Random forest
  rf = randomForest(x = as.matrix(X[train, ]), y = y[train] )
  
  ## Linear regression
  df_lin = as.data.frame(X) # Create df for lm function
  df_lin$y = y
  
  model_linear = lm(y ~ ., data = df_lin[train, ])
  
  ## XGboost
  # Convert your data to a DMatrix (recommended for xgboost)
  dtrain = xgb.DMatrix(data = as.matrix(X[train, ]), label = y[train])

  # Set parameters
  params = list(
    objective = "reg:squarederror",  # for regression
    lambda = 1,   # L2 regularization (ridge)
    alpha = 1     # L1 regularization (lasso)
  )
  
  # Fit the model with default parameters + regularization
  model_xgb = xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,   # number of boosting rounds (iterations), can tune this
    verbose = 1      # print progress
  )
  
  ## SVM
  model_svm = svm(
    x = X[train, ],    # training predictors
    y = y[train],      # training response
    type = "eps-regression" # regression type (default is epsilon-regression)
  )
  
  
  ##################################################
  
  # Model Evaluation
  
  ## True signal
  # Make prediction
  true_signal_pred = true_signal_all(X[test, ])
  # Evaluate
  metrics.true_signal = get_all_6_metrics(test_Y = y[test], test_Y_Hat = true_signal_pred)
  
  ## Null model
  # Make predictions
  dummy_pred_r2 = rep(mean(y[train]), length(test))
  # Evaluate predictions
  metrics.null_R2 = get_all_6_metrics(test_Y = y[test], test_Y_Hat = dummy_pred_r2)
  
  ## Elatic net
  # Make predictions
  elastic_pred = stats::predict(model_elastic, newx = X[test, ])
  # Evaluate predictions
  metrics.elastic = get_all_6_metrics(test_Y = y[test], test_Y_Hat = elastic_pred)
  
  ## Random forest
  # Make predictions
  rf_pred = stats::predict(rf, newdata = as.matrix(X[test, ] ))
  # Make predictions
  rf_pred = stats::predict(rf, newdata = as.matrix(X[test, ] ))
  # Evaluate predictions
  metrics.rf = get_all_6_metrics(test_Y = y[test], test_Y_Hat = rf_pred)
  
  ## Linear regression
  # Make predictions
  lin_data_X = subset(df_lin, select = -c(y))
  lm_pred = stats::predict(model_linear, newdata = lin_data_X[test, ] )
  # Evaluate predictions
  metrics.linear = get_all_6_metrics(test_Y = y[test], test_Y_Hat = lm_pred)
  
  ## XGboost
  # Make predictions
  xgboost_pred = predict(model_xgb, newdata = as.matrix(X[test, ]))
  # Evaluate predictions
  metrics.xgb = get_all_6_metrics(test_Y = y[test], test_Y_Hat = xgboost_pred)
  
  ## SVM
  # Make predictions
  svm_pred = predict(model_svm, newdata = X[test, ])
  # Evaluate predictions
  metrics.svm = get_all_6_metrics(test_Y = y[test], test_Y_Hat = svm_pred)
  
  ##################################################
  
  # Model Ratios
  
  ## True signal
  ratio.true_signal.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE)  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  ratio.elastic.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE)  
  
  ## Random forest
  ratio.rf.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE) 
  
  ## Linear regression
  ratio.linear.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE)  
  
  ## XGboost
  ratio.xgb.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE)  
  
  ## SVM
  ratio.svm.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE)
  
    ##################################################
  
  # Model Min-Max
  
  ## True signal
  min_max.true_signal.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE)  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  min_max.elastic.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE)  
  
  ## Random forest
  min_max.rf.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE) 
  
  ## Linear regression
  min_max.linear.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE)  
  
  ## XGboost
  min_max.xgb.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE)  
  
  ## SVM
  min_max.svm.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE)  
  
  ##################################################
  
  # ND-RMSE

  ## True signal
  nd_rmse.true_signal.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE,
                              test_Y = y[test])  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  nd_rmse.elastic.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE,
                              test_Y = y[test])  
  
  ## Random forest
  nd_rmse.rf.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE,
                             test_Y = y[test]) 
  
  ## Linear regression
  nd_rmse.linear.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE,
                              test_Y = y[test])  
  
  ## XGboost
  nd_rmse.xgb.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE,
                              test_Y = y[test])  
  
  ## SVM
  nd_rmse.svm.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE,
                              test_Y = y[test])  

  
  ##################################################
  
  # Summary
  df_summary = data.frame(
    Metric = c(names(metrics.true_signal), "PR", "MinMax", "NDRMSE"),
    True_signal = c(unname(unlist(metrics.true_signal)), ratio.true_signal.R2, min_max.true_signal.R2, nd_rmse.true_signal.R2),
    Dummy_R2 = c(unname(unlist(metrics.null_R2)), NA, NA, NA),
    elastic_net = c(unname(unlist(metrics.elastic)), ratio.elastic.R2, min_max.elastic.R2, nd_rmse.elastic.R2),
    RF = c(unname(unlist(metrics.rf)), ratio.rf.R2, min_max.rf.R2, nd_rmse.rf.R2),
    Linear_reg = c(unname(unlist(metrics.linear)), ratio.linear.R2, min_max.linear.R2, nd_rmse.linear.R2),
    XGB = c(unname(unlist(metrics.xgb)), ratio.xgb.R2, min_max.xgb.R2, nd_rmse.xgb.R2),
    SVM = c(unname(unlist(metrics.svm)), ratio.svm.R2, min_max.svm.R2, nd_rmse.svm.R2)
  )
  
  # Pivot all columns except for the "metric" column into a new column called "model"
  # The values go into the values column
  df_long = df_summary %>%
    pivot_longer(
      cols = -Metric,
      names_to = "model",
      values_to = "value")

  # Pivot each metric name from the metric column into its own column
  df_wide = df_long %>%
    pivot_wider(
      names_from = Metric,
      values_from = value)
  
  df_final = df_wide %>%
    arrange(desc(PR))
  
  ##################################################
  
  # # Memory clean up
  # objects_to_keep = "df_final"
  # all_objects = ls() # List all environments inside this function
  # to_remove = setdiff(all_objects, objects_to_keep)
  # #print(to_remove)
  # rm(list = to_remove) # Remove
  # 
  # # Force garbage collection
  # gc(verbose = F)
  # invisible(gc(full = TRUE, verbose = F))
  
  # Final function output
  return(df_final)
}


# Sanity check
df_Dutch_summary = generate_summary_table(X = df_Dutch$X, y = df_Dutch$y, signal_type = signal_type_global_NL)
df_Dutch_summary

df_German_summary = generate_summary_table(X = df_German$X, y = df_German$y, signal_type = signal_type_global_DE)
df_German_summary
```



## Rank Correlation

**FUNCTION EXPLANATION**: `get_rank_cor`  

**Args:**  
1. **df_Dutch** (data frame): performance metrics for models evaluated on the Dutch dataset -> Reference.  
2. **df_German** (data frame): performance metrics for models evaluated on the German dataset -> Non-reference.  
3. **exclude_model** (character vector): models to exclude from ranking comparison (default: `c("True_signal", "Dummy_R2")`).  
4. **metrics_list** (character vector): list of metrics to evaluate rank correlation for (default: `c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE")`).  
5. **higher_is_better_list** (named logical vector): for each metric, `TRUE` if higher values indicate better performance, `FALSE` otherwise. Defaults:  
   - NSE = TRUE, RSE = FALSE, RRMSE = FALSE, NRMSE = FALSE,  
     MAPE = FALSE, PR = TRUE, MSE = FALSE, MinMax = TRUE, NDRMSE = TRUE.  

**Returns:**  
- A named numeric vector of Kendall rank correlations between the “true” ranking (based on Dutch MSE) and the proposed rankings from the German dataset, for each metric in `metrics_list`.  

**Main logic:**  
1. Remove excluded models from the Dutch dataset.  
2. Determine the **true ranking** of models using Dutch data, sorting by lowest MSE.  
   - Assign **fixed reference IDs** \(1, 2, \dots, n\) to these models (each ID corresponds to a specific model).  
3. Remove excluded models from the German dataset.  
4. For each metric in `metrics_list`:  
   - Sort German models by the metric, in ascending or descending order depending on `higher_is_better_list`.  
   - Convert the ordered German model names into **IDs** using the Dutch reference mapping.  
   - These IDs represent the **German ranking in the Dutch ID system**.  
   - Compute Kendall rank correlation between the Dutch fixed IDs and the mapped German IDs.  
5. Return a vector of correlations, named by metric.  

**Example of mapping logic:**  
- Dutch ranking: `["ModelA", "ModelB", "ModelC"]` -> ranks: `1, 2, 3`  
- German ranking: `["ModelC", "ModelA", "ModelB"]` -> Mapped German rankings will be `3, 1, 2`  
- Kendall correlation is computed between `1, 2, 3` and `3, 1, 2`. 


```{r}
get_rank_cor = function(df_Dutch, df_German, exclude_model = c("True_signal", "Dummy_R2"),
                        metrics_list = c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE"),
                        higher_is_better_list = c(NSE = TRUE, RSE = FALSE, RRMSE = FALSE, NRMSE = FALSE, 
                                                       MAPE = FALSE, PR = TRUE, MSE = FALSE, MinMax = TRUE, NDRMSE = TRUE)) {
  # Step 1: True ranking -> Based on dutch$MSE
  df_Dutch_5_models = df_Dutch %>% filter(! model %in% exclude_model)
  
  true_ranking_names = df_Dutch_5_models[order(df_Dutch_5_models$MSE), ] %>% pull(model)
  true_ranking_idx = 1:length(true_ranking_names)
  
  print("True ranking list")
  print(true_ranking_names)
  
  # Step 2: Proposed ranking
  df_German_5_models = df_German %>% filter(! model %in% exclude_model)
  
  # Initialize results vector
  results = vector("numeric", length = length(metrics_list))
  names(results) = metrics_list
  
  # Compute rank correlation for each metric
  for (metric in metrics_list) {
    proposed_ranking_names = df_German_5_models[order(df_German_5_models[[metric]], decreasing = higher_is_better_list[metric]), ] %>% 
      pull(model)
    
    proposed_ranking_idx = match(true_ranking_names, proposed_ranking_names)
    
    print(paste("Proposed ranking for -> ", metric))
    print(proposed_ranking_names)
    print(proposed_ranking_idx)
    
    # Compute rank correlation
    results[metric] = cor(true_ranking_idx, proposed_ranking_idx, method = "kendall")
  }
  
  return(results)
  
}

# Linear reg, elastic net, svm, rf, xgb <= Dutch
# RF, xgb, svm, linear reg, elastic net <= German
get_rank_cor(df_Dutch_summary, df_German_summary, metrics_list = c("MSE"), higher_is_better_list = c("MSE"=F))


```


```{r}
ranks = get_rank_cor(df_Dutch_summary, df_German_summary)
ranks

data.frame(metric = names(ranks), ranks = unname(ranks))
```





##  Relative Difference

This function is a historic artifact. It was originally used to compute the relative difference between two metric values before we formalized the EPE lower bound constraints.

```{r}
RD_smape = function(mk_A, mk_B) {
  if (mk_A == 0 && mk_B == 0) {
    return(0)
  } else {
    max_mk = max(mk_A, mk_B)
    min_mk = min(mk_A, mk_B)
    
    tmp = (max_mk - min_mk) / min(abs(mk_A), abs(mk_B) ) # Difference relative to larger magnitude
    1 - exp(-tmp)
  }
}

RD_arnout = function(mk_A, mk_B) {
  if (mk_A == 0 && mk_B == 0) {
    return(0)
  } else if (abs(mk_A) == mk_B || mk_A == abs(mk_B)) {
    return(NaN)
  } else {
    abs(  (mk_A - mk_B) / ( (mk_A+mk_B)/2) )
  }
}


RD_arnout(0.07946066, 0.06019395) # SVM improvement
RD_arnout(15, 5) # LM degen
```


Get single RD

```{r}
calculate_single_RD = function(df_Dutch, df_German,
                                missing_model, metric_name) {
  # Extract score for metric m, algorithm k
  mk_A = df_Dutch %>% 
    filter(model == missing_model) %>% 
    pull(metric_name)
  
  mk_B = df_German %>% 
    filter(model == missing_model) %>% 
    pull(metric_name)
  
  RD_arnout(mk_A, mk_B)
}

# Sanity check
calculate_single_RD(df_Dutch = df_Dutch_summary,
                     df_German = df_German_summary,
                     missing_model = "XGB", metric_name = "NSE")
```



```{r}
calculate_all_RD_return_mean = function(df_Dutch, df_German, 
                              metrics_list = c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE")) {
  
  # Step 1: Get model names to iterate over
  model_name = df_Dutch_summary %>% 
    filter(! model %in% c("True_signal", "Dummy_R2")) %>% 
    pull(model)
  
  # List of metrics to evaluate
  metrics = metrics_list
  
  # Initialize results matrix
  results = matrix(ncol = length(metrics), nrow = length(model_name))
  row.names(results) = model_name
  colnames(results) = metrics
  
  
  for (model in model_name) {
    for (metric in metrics) {
      results[model, metric] = calculate_single_RD(
        df_Dutch = df_Dutch,
        df_German = df_German,
        missing_model = model,
        metric_name = metric
      )
    }
  }
  
  final = results %>% 
    apply(MARGIN = 2, FUN = mean)
  
  print(data.frame(results))

  return(final)
}

# Sanity check
RD_vector = calculate_all_RD_return_mean(df_Dutch = df_Dutch_summary,
                   df_German = df_German_summary)

RD_vector

# Sanity check
calculate_single_RD(df_Dutch = df_Dutch_summary,
                     df_German = df_German_summary,
                     missing_model = "SVM", metric_name = "MSE")
```

```{r}
library(tidyr)
```


```{r}
calculate_all_RD = function(df_Dutch, df_German, 
                              metrics_list = c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE")) {
  
  # Step 1: Get model names to iterate over
  model_name = df_Dutch_summary %>% 
    filter(! model %in% c("True_signal", "Dummy_R2")) %>% 
    pull(model)
  
  # List of metrics to evaluate
  metrics = metrics_list
  
  # Initialize results matrix
  results = matrix(ncol = length(metrics), nrow = length(model_name))
  row.names(results) = model_name
  colnames(results) = metrics
  
  
  for (model in model_name) {
    for (metric in metrics) {
      results[model, metric] = calculate_single_RD(
        df_Dutch = df_Dutch,
        df_German = df_German,
        missing_model = model,
        metric_name = metric
      )
    }
  }
  
  # Convert to long format
  final = data.frame(results) %>%
    tibble::rownames_to_column(var = "model") %>%
    pivot_longer(
      cols = -model,
      names_to = "metric",
      values_to = "score")

  return(final)
}


tmp = calculate_all_RD(df_Dutch = df_Dutch_summary, df_German = df_German_summary)
tmp

# Sanity check
tmp1 = tmp %>% group_by(metric) %>%
  summarise(mean_score_over_5_models = mean(score))
tmp1


sort(tmp1$mean_score_over_5_models) == sort(RD_vector)
```





# #######################

# 3. Main Simulation Loop

Progress bar
```{r}
library(progress)
```

**CODE EXPLANATION**: Nested simulation over signal types  

**Purpose:**  
Repeat the experiment **`B_global` times** to account for sampling variance, while systematically varying only the signal type for the German dataset (non-reference).  
- **Outer loop**: Represents one full replication (ie. MC run) of the experiment -> Dutch dataset (reference) is held fixed.
- **Inner loop**: Iterates over different signal types for the German dataset while keeping all other parameters fixed.  

```{r}
# Timing the code
start.time = Sys.time()

# Initialize an empty dataframe to store all results
all_results_df = data.frame()
sim_model_results_df = data.frame()
rank_cor_sim_df = data.frame()

# Progress bar
pb = progress_bar$new(format = "(:spin) [:bar] :percent [Elapsed time: :elapsedfull || Estimated time remaining: :eta]",
                       total = B_global * n_signal_global,
                       complete = "=",   # Completion bar character
                       incomplete = "-", # Incomplete bar character
                       current = ">",    # Current bar character
                       clear = FALSE,    # If TRUE, clears the bar when finish
                       width = 100)      # Width of the progress bar

for (idx in 1:B_global) {
  # Reference dataset stays fixed
  df_Dutch = generate_data( use_original_signal_var = FALSE, noise = noise_global_NL, type = signal_type_global_NL)
  df_Dutch_summary = generate_summary_table(X = df_Dutch$X, y = df_Dutch$y, signal_type = signal_type_global_NL)
  
  for (signal_type in 1:n_signal_global) {
    # Generate datasets
    df_German = generate_data( use_original_signal_var = FALSE, noise = noise_global_DE, type = signal_type)

    # Generate summary tables
    df_German_summary = generate_summary_table(X = df_German$X, y = df_German$y, signal_type = signal_type)
    
    # Model performance over curvature ###########
    # Convert to longer format
    df_DE_longer = df_German_summary %>% 
      pivot_longer(cols = -model, names_to = "metric", values_to = "score")
    df_DE_longer$signal_type = signal_type
    df_DE_longer$simulation = idx
    sim_model_results_df = rbind(sim_model_results_df, df_DE_longer)

    # Compute RD metrics  ###########
    RD_df = calculate_all_RD(df_Dutch = df_Dutch_summary, df_German = df_German_summary)
    
    # Add noise and simulation iteration columns
    RD_df$signal_type = signal_type
    RD_df$simulation = idx
    
    # Append to the main results dataframe
    all_results_df = rbind(all_results_df, RD_df)
    
    # Compute rank correlation   ###########
    rank_cor = get_rank_cor(df_Dutch_summary, df_German_summary)
    rank_cor_df = data.frame(metric = names(rank_cor), rank_cor = unname(rank_cor))
    rank_cor_df$signal_type = signal_type
    rank_cor_df$simulation = idx
    
    rank_cor_sim_df = rbind(rank_cor_sim_df,rank_cor_df )
    
    pb$tick()
  }
}
end.time = Sys.time()
time.taken = end.time - start.time
time.taken


# Show results
all_results_df
```

Save results so that I don't have to re-run simulations all the time.

```{r}
# Step 1: Create a timestamp
timestamp = format(Sys.time(), "%Y-%m-%d_%H-%M-%S")

filename_ranks = paste0(
  "ranks_",
  timestamp,
  "_ref_signaltype=", signal_type_global_NL,
  "_refNoise=", noise_global_NL,
  "_B=", B_global,
  ".Rdata"
)

filename_metric_scores = paste0(
  "metric-scores_",
  timestamp,
  "_ref_signaltype=", signal_type_global_NL,
  "_refNoise=", noise_global_NL,
  "_B=", B_global,
  ".Rdata"
)

# Save files
save(rank_cor_sim_df, file = filename_ranks)
save(all_results_df, file = filename_metric_scores)
```






# #######################

# 4. Visualization

```{r}
load("ranks_2025-07-15_15-35-44_ref_signaltype=2_refNoise=0.1_B=50.Rdata")
```



```{r}
library(ggplot2)
library(dplyr)
library(scales)
library(ggrepel)
```


## Relative difference

Global visualization settings: Subtitle and color encoding

```{r}
# Create a vector of all possible metrics
all_possible_metrics = c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE")  # Include all potential metrics

# Get default ggplot colors for all metrics
metric_colors = setNames(hue_pal()(length(all_possible_metrics)), all_possible_metrics)
metric_colors
```

Get caption

```{r}
# Get the function body
fun_body = body(true_signal)

# Extract expressions for each of the 4 cases (skip the default)
signal_formulas = sapply(1:4, function(i) {
  expr = fun_body[[2]][[4]][[2]][[2 + i]]  # Your corrected indexing
  deparse(expr) |> paste(collapse = "")    # Collapse into single-line string
})

# Define the correct label order
curvature_labels = c("None", "Low", "Mid", "High")

# Build signal mapping tibble with correct factor levels
signal_mapping = tibble::tibble(
  signal_type = 1:4,
  curvature_label = factor(curvature_labels, levels = curvature_labels),
  function_str = signal_formulas
)

# Group into lines: 2 per line
caption_lines = signal_mapping %>%
  mutate(line_group = ceiling(row_number() / 2)) %>%
  group_by(line_group) %>%
  summarise(
    text = paste0(
      curvature_label[1], ": ", function_str[1], "     ",
      if (n() > 1) paste0(curvature_label[2], ": ", function_str[2]) else ""
    ),
    .groups = "drop"
  ) %>%
  pull(text)

# Final caption string
caption_text = paste(caption_lines, collapse = "\n")

# summary_df = summary_df %>%
#   left_join(signal_mapping, by = "signal_type")
```



```{r}
my_subtitle = paste(
      "NL & DE Noise proportion fixed at: ", noise_global_NL,
      "\nSample size per covariate: ", n_global / ncol(df_Dutch$X),
      "; Number of MC simulations: ", B_global,
      ";\nDE cuvature (varying) = {None, Low, Mid, High}",
      ";\nNL cuvature (fixed) = {None}", 
      sep = ""
    )

my_subtitle
```



```{r }
# First compute the mean score # fig.width=10, fig.height=4
mean_results_df = all_results_df %>%
  group_by(model, metric, signal_type) %>%
  summarise(mean_score = mean(score), .groups = "drop")

# mean_results_df$metric = factor(mean_results_df$metric, 
#                                  levels = c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax"))

# Plot
ggplot(mean_results_df, aes(x = signal_type, y = mean_score, color = metric)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ model, nrow = 1) +
  scale_color_manual(values = metric_colors) +
  labs(x = "signal_type Level", y = "RD", color = "Metric",
       title = "RD over signal_type levels",
       subtitle = my_subtitle) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Get the last noise level per model/metric combo
label_points = mean_results_df %>%
  group_by(model, metric) %>%
  filter(signal_type == max(signal_type)) %>%
  ungroup()

ggplot(mean_results_df, aes(x = signal_type, y = mean_score, color = metric)) +
  geom_line() +
  geom_point() +
  geom_text_repel(
    data = label_points,
    aes(label = metric),
    direction = "y",
    hjust = 0.1,
    nudge_x = 0.5,
    segment.color = 'grey50',
    size = 3,
    show.legend = FALSE
  ) +
  facet_wrap(~ model, nrow = 1) +
  labs(
    x = "signal_type Level", y = "RD", color = "Metric",
    title = "RD over signal_type levels",
    subtitle = my_subtitle,
    caption = paste("Signal Definitions:\n", caption_text)
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

Since we have overlaps, maybe the plot will look less cluttered if we removed NSE and RSE.

```{r}
removed_NSE_RSE = mean_results_df %>% 
  filter(! metric %in% c("NSE", "RSE"))

# Get the last noise level per model/metric combo for labels
label_points = removed_NSE_RSE %>%
  group_by(model, metric) %>%
  filter(signal_type == max(signal_type)) %>%
  ungroup()

# Create the plot
ggplot(removed_NSE_RSE, aes(x = signal_type, y = mean_score, color = metric)) +
  geom_line() +
  geom_point() +
  geom_text_repel(
    data = label_points,
    aes(label = metric),
    direction = "y",
    hjust = 0.5,
    nudge_x = 0.9,
    segment.color = 'grey50',
    size = 3,
    show.legend = FALSE
  ) +
  facet_wrap(~ model, nrow = 1) +
  scale_color_manual(values = metric_colors) +  # Use the default ggplot colors
  labs(
    x = "signal_type Level", y = "RD", color = "Metric",
    title = "RD over noise levels",
    subtitle = my_subtitle,
    caption = paste("Signal Definitions:\n", caption_text)
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

Per simulation and per noise, average over all algorithms.

```{r}
avg_over_modesl_df = all_results_df %>% 
  group_by(metric, simulation, signal_type) %>%
  summarise(mean_score_over_5_models = mean(score))

avg_over_modesl_df
```

Connecting the old with the new code

```{r}
# Connecting the old with the new code
long_df = avg_over_modesl_df
long_df$score = long_df$mean_score_over_5_models
```


```{r}
# Compute means per metric and noise
summary_df = long_df %>%
  group_by(signal_type, metric) %>%
  summarize(mean_score = mean(score, na.rm = TRUE),
            sd_score = sd(score, na.rm = TRUE),
            sd_lower = mean_score - sd_score,
            sd_upper = mean_score + sd_score,
            .groups = "drop")
```


```{r}
ggplot(summary_df, aes(x = signal_type, y = mean_score, color = metric)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    x = "signal_type Level",
    y = "Relative Difference (RD) between mk_NL and mk_DE",
    title = "RD vs Noise Proportion by Metric",
    subtitle = my_subtitle,
  ) +
  theme_minimal()


ggplot(summary_df, aes(x = signal_type, y = mean_score, color = metric, fill = metric)) +
  geom_ribbon(aes(ymin = sd_lower, ymax = sd_upper), alpha = 0.2, color = NA) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    x = "signal_type Level",
    y = "RD",
    title = "RD vs Noise Proportion by Metric (±1 SD bands)",
    subtitle = my_subtitle,
    caption = paste("Signal Definitions:\n", caption_text)
  ) +
  theme_minimal()
```



```{r}
library(ggrepel)

# Get the last point for each metric
label_df = summary_df %>%
  group_by(metric) %>%
  filter(signal_type == max(signal_type))  # or slice_max(noise)

ggplot(summary_df, aes(x = signal_type, y = mean_score, color = metric)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_text_repel(
    data = label_df,
    aes(label = metric),
    direction = "y",
    hjust = 0.01,
    nudge_x = 0.05,
    segment.color = 'grey50',
    show.legend = FALSE
  ) +
  scale_color_manual(values = metric_colors) +
  labs(
    x = "signal_type proportion",
    y = "Mean Relative Difference (over 5 models)",
    title = "RD vs signal_type Level by Metric",
    subtitle = my_subtitle
  ) +
  theme_minimal() +
  theme(legend.position = "none")


# With SD
ggplot(summary_df, aes(x = signal_type, y = mean_score, color = metric, fill = metric)) +
  geom_ribbon(aes(ymin = sd_lower, ymax = sd_upper), alpha = 0.2, color = NA) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_text_repel(
    data = label_df,
    aes(label = metric),
    direction = "y",
    hjust = 0.01,
    nudge_x = 0.05,
    segment.color = 'grey50',
    show.legend = FALSE
  ) +
  scale_color_manual(values = metric_colors) +
  scale_fill_manual(values = metric_colors) +
  labs(
    x = "signal_type Level",
    y = "Mean Relative Difference (over 5 models)",
    title = "RD vs signal_type Level by Metric (±1 SD bands)",
    subtitle = my_subtitle,
    caption = paste("Signal Definitions:\n", caption_text)
  ) +
  theme_minimal() +
  theme(legend.position = "none")


```


NSE, RSE removed

```{r}
aggregated_NSE_RSE_removed = summary_df %>% 
  filter(! metric %in% c("NSE", "RSE"))


label_df = aggregated_NSE_RSE_removed %>%
  group_by(metric) %>%
  filter(signal_type == max(signal_type)) 

ggplot(aggregated_NSE_RSE_removed, aes(x = signal_type, y = mean_score, color = metric, fill = metric)) +
  geom_ribbon(aes(ymin = sd_lower, ymax = sd_upper), alpha = 0.2, color = NA) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_text_repel(
    data = label_df,
    aes(label = metric),
    direction = "y",
    hjust = 0.25,
    nudge_x = 0.5,
    segment.color = 'grey50',
    show.legend = FALSE
  ) +
  scale_color_manual(values = metric_colors) +
  scale_fill_manual(values = metric_colors) +
  labs(
    x = "Curvature Level",
    y = "Mean Relative Difference (over 5 models)",
    title = "RD vs Curvature Level by Metric (±1 SD bands)",
    subtitle = my_subtitle,
    caption = paste("Signal Definitions:\n", caption_text)
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Individual algorithms

Check how to visualize a single simulation.

```{r}
sim_1 = sim_model_results_df %>% 
  filter(simulation == 1)

ggplot(data = sim_1, mapping = aes(x = signal_type, y = score, color = model)) +
  geom_line(na.rm = T) +
  facet_wrap(~ metric, scales = "free_y")

# NO DUMMY  
sim_1_no_dummy = sim_model_results_df %>% 
  filter(simulation == 1, model != "Dummy_R2")

ggplot(data = sim_1_no_dummy, mapping = aes(x = signal_type, y = score, color = model)) +
  geom_line(na.rm = T) +
  facet_wrap(~ metric, scales = "free_y") +
  labs(title = "Sim 1")

# ONLY RF, Linear, True
sim_1_rf_lin_true = sim_model_results_df %>% 
  filter(simulation == 1, model %in% c("Linear_reg", "XGB", "True_signal"))

ggplot(data = sim_1_rf_lin_true, mapping = aes(x = signal_type, y = score, color = model)) +
  geom_line(na.rm = T) +
  facet_wrap(~ metric, scales = "free_y")
```

```{r}
sim_1_MSE = sim_model_results_df %>% 
  filter(simulation == 1, model != "Dummy_R2", metric == "MSE")

ggplot(sim_1_MSE, aes(x = signal_type, y = score, color = model)) +
  geom_line()
```

Aggregate over all simulations.

```{r}
# Summarize: mean and sd of score over simulations
summary_df = sim_model_results_df %>%
  # filter(model %in% c("Linear_reg", "RF", "True_signal", "Dummy_R2")) %>%
  group_by(model, metric, signal_type) %>%
  summarise(
    mean_score = mean(score, na.rm = TRUE),
    sd_score = sd(score, na.rm = TRUE),
    .groups = "drop"
  )

# Plot: with ribbon for ± SD
ggplot(summary_df %>% filter(! model %in% c("Dummy_R2")), 
       aes(x = signal_type, y = mean_score, color = model, fill = model)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score), 
              alpha = 0.2, color = NA) +
  facet_wrap(~ metric, scales = "free_y") +
    labs(
    title = "Model Performance Across Signal Types",
    subtitle = paste("Mean score ± 1 SD over", B_global, "simulations"),
    x = "Signal Type",
    y = "Score",
    color = "Model",
    fill = "Model"
  ) +
  theme_minimal()

```

```{r}
# Get the body of the function
fun_body = body(true_signal)

for (i in 1:4) {
  switch_exprs = fun_body[[2]][[4]][[2]][[2+i]] 
  i = i + 1
  print(switch_exprs)
}


```

```{r}
# Summarize: mean and sd of score over simulations
summary_df = sim_model_results_df %>%
  filter(model %in% c("Linear_reg", "XGB", "True_signal")) %>%
  group_by(model, metric, signal_type) %>%
  summarise(
    mean_score = mean(score, na.rm = TRUE),
    sd_score = sd(score, na.rm = TRUE),
    .groups = "drop"
  )

# Get the function body
fun_body = body(true_signal)

# Extract expressions for each of the 4 cases (skip the default)
signal_formulas = sapply(1:4, function(i) {
  expr = fun_body[[2]][[4]][[2]][[2 + i]]  # Your corrected indexing
  deparse(expr) |> paste(collapse = "")    # Collapse into single-line string
})

# Define the correct label order
curvature_labels = c("Linear", "Quadratic", "Exp + low freq", "Exp + high Freq")

# Build signal mapping tibble with correct factor levels
signal_mapping = tibble::tibble(
  signal_type = 1:4,
  curvature_label = factor(curvature_labels, levels = curvature_labels),
  function_str = signal_formulas
)

# Group into lines: 2 per line
caption_lines = signal_mapping %>%
  mutate(line_group = ceiling(row_number() / 2)) %>%
  group_by(line_group) %>%
  summarise(
    text = paste0(
      curvature_label[1], ": ", function_str[1], "     ",
      if (n() > 1) paste0(curvature_label[2], ": ", function_str[2]) else ""
    ),
    .groups = "drop"
  ) %>%
  pull(text)

# Final caption string
caption_text = paste(caption_lines, collapse = "\n")

summary_df = summary_df %>%
  left_join(signal_mapping, by = "signal_type")


# Fix legend
selected_model_labels = c("True_signal" = "True Function",
                 "Linear_reg" = "Linear Regression",
                 "XGB" = "XGBoost")


ggplot(summary_df, aes(x = curvature_label, y = mean_score, color = model, fill = model, group = model)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score), 
              alpha = 0.2, color = NA, show.legend = F) +
  facet_wrap(~ metric, scales = "free_y") +
  labs(
    title = "Model Performance Across Signal Curvature Levels",
    subtitle = paste0("Mean score ± 1 SD over ", B_global, " simulations, with noise proportion ", 
                      noise_global_DE),
    caption = paste("Signal Definitions:\n", caption_text),
    x = "Signal Curvature",
    y = "Score",
    color = "Model",
    fill = "Model"
  ) +
  scale_color_discrete(labels = selected_model_labels,
                     name = "Selected Models") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    plot.caption = element_text(hjust = 0, face = "italic", size = 9)
  )

```

### Thesis plots

```{r}
# Fix legend
selected_model_labels = c("True_signal" = "Oracle",
                 "Linear_reg" = "Linear Regression",
                 "XGB" = "XGBoost")


algo_plot = ggplot(summary_df %>% filter(metric %in% c("MSE", "NSE", "NRMSE", "RRMSE", "PR")), 
       aes(x = curvature_label, y = mean_score, color = model, fill = model, group = model)) +
  geom_line() +
  geom_point(show.legend = F, size = 0.6) +
  geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score), 
              alpha = 0.2, color = NA, show.legend = F) +
  facet_wrap(~ metric, scales = "free_y") +
  labs(
    title = "Performance Across Signal Types and Algorithms",
    subtitle = paste0("Mean performance ± 1 SD over ", B_global, " Monte Calro simulations"),
    x = "Signal Type",
    y = "Metric Performance Score",
    color = "Model",
    fill = "Model"
  ) +
  scale_color_discrete(labels = selected_model_labels,
                     name = "Selected Algorithm") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
  )

algo_plot

```
```{r, dpi=300, fig.width=8, fig.height=5}
algo_plot
```


## Ranks

```{r}
sim_1 = rank_cor_sim_df %>% 
  filter(simulation == 1)

ggplot(data = sim_1, mapping = aes(x = signal_type, y = rank_cor, color = metric)) +
  geom_point(na.rm = T) +
  facet_wrap(~ metric, scales = "free_y")

```

```{r}
# Summarize: mean and sd of ranks correlation over simulations
summary_df = rank_cor_sim_df %>%
  group_by(metric, signal_type) %>%
  summarise(
    mean_rank_cor = mean(rank_cor, na.rm = TRUE),
    sd_rank_cor = sd(rank_cor, na.rm = TRUE),
    q5 = quantile(rank_cor, 0.05),
    q95 = quantile(rank_cor, 0.95),
    .groups = "drop"
  )

summary_df

# Plot: with ribbon for ± SD
ggplot(summary_df, aes(x = signal_type, y = mean_rank_cor, color = metric, fill = metric)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean_rank_cor - sd_rank_cor, 
                  ymax = mean_rank_cor + sd_rank_cor),
              width = 0.3) + 
  geom_ribbon(aes(ymin = mean_rank_cor - sd_rank_cor, ymax = mean_rank_cor + sd_rank_cor), 
              alpha = 0.2, color = NA) +
  facet_wrap(~ metric) +
    labs(
    title = "Rank Correlation Across Signal Curvature Levels",
    subtitle = paste("Mean rank correlation ± 1 SD over", B_global, "simulations",
                     "\nReference curvature level =", signal_type_global_NL),
    x = "Signal Curvature",
    y = "Rank Correlation",
    color = "Model",
    caption = paste("Signal Definitions:\n", caption_text),
    fill = "Model"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(-1.2, 1.2)) +
    theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    plot.caption = element_text(hjust = 0, face = "italic", size = 9)
  )
```

```{r}
# Plot: with ribbon for 90% CI
ggplot(summary_df, aes(x = signal_type, y = mean_rank_cor, color = metric, fill = metric)) +
  geom_point() +
  geom_errorbar(aes(ymin = q5, 
                  ymax = q95),
              width = 0.3) + 
  geom_ribbon(aes(ymin = q5, ymax = q95), 
              alpha = 0.2, color = NA) +
  facet_wrap(~ metric) +
    labs(
    title = "Rank Correlation Across Signal Types",
    subtitle = paste("Mean rank correlation 90% empirical confidence interval over", B_global, "simulations",
                     "\nReference signal type =", signal_type_global_NL),
    x = "Signal Type",
    y = "Rank Correlation",
    color = "Model",
    caption = paste("Signal Definitions:\n", caption_text),
    fill = "Model"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(-1.2, 1.2)) +
  theme(
    plot.caption = element_text(hjust = 0, face = "italic", size = 9)
  )


# Plot: with ribbon for 90% CI
ggplot(summary_df %>% filter(metric == "MSE"), 
       aes(x = signal_type, y = mean_rank_cor, color = metric, fill = metric)) +
  geom_point() +
  geom_errorbar(aes(ymin = q5, 
                  ymax = q95),
              width = 0.3) + 
  geom_ribbon(aes(ymin = q5, ymax = q95), 
              alpha = 0.2, color = NA) +
  # facet_wrap(~ metric) +
    labs(
    title = "Rank Correlation Across Signal Types",
    subtitle = paste("Mean rank correlation 90% empirical confidence interval over", B_global, "simulations",
                     "\nReference signal type =", signal_type_global_NL, ";",
                     "Noise proportion =", noise_global_NL),
    x = "Signal Type",
    y = "Rank Correlation (MSE based)",
    color = "Model",
    caption = paste("Signal Definitions:\n", caption_text),
    fill = "Model"
  ) +
  geom_hline(yintercept = 0.4, color = "green") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(-1.2, 1.2)) +
  theme(
    plot.caption = element_text(hjust = 0, face = "italic", size = 9)
  )
```


```{r}
summary_df %>% 
  filter(metric == "MSE")
```

Plot RC distribution

```{r}
# Histogram
rank_cor_sim_df %>% filter(signal_type == 4, metric == "MSE") %>% select(rank_cor) %>% 
  histograms() + labs(title = "Rank correlation for signal type 4, over 50 MC iterations")

# Density
rank_cor_sim_df %>% filter(signal_type == 4, metric == "MSE") %>% select(rank_cor) %>% 
  unlist() %>% unname() %>% density() %>% plot

# Quantile 0.05
rank_cor_sim_df %>% filter(signal_type == 4, metric == "MSE") %>% select(rank_cor) %>% 
  unlist() %>% unname() %>%  quantile(0.05)

# Quantile 0.95
rank_cor_sim_df %>% filter(signal_type == 4, metric == "MSE") %>% select(rank_cor) %>% 
  unlist() %>% unname() %>%  quantile(0.95)


# Proportion of MC iterations where the rank correlation is >=0.4
rank_cor_sim_df %>% 
  filter(signal_type == 4, metric == "MSE") %>% 
  pull(rank_cor) %>% 
  {mean(. >= 0.4)}
```
### Thesis plots

```{r}
viz_df = rank_cor_sim_df %>% filter(metric == "MSE")

# Order by tau (increasing) within each group
viz_df = viz_df %>% group_by(signal_type) %>% 
  arrange(signal_type, rank_cor) %>% 
  mutate(id_per_group = row_number())

# Color the bottom 3 draws (6% of 50) within each group blue, the rest grey
n_outliers = ceiling(0.05 * B_global)  # Bottom 3 draws = 6% of 50 draws
viz_df = viz_df %>% 
  mutate(outliers = ifelse(id_per_group %in% 1:n_outliers, "Bottom 3", "Top 47"))

# Convert sample_size_per_cov to factor for discrete x-axis
viz_df$signal_type_factor = factor(viz_df$signal_type, 
                                           levels = sort(unique(viz_df$signal_type)))

# Filter data for each layer
grey_data = viz_df %>% filter(outliers == "Top 47")
blue_data = viz_df %>% filter(outliers == "Bottom 3")

# Create boxplot version
p_boxplot = ggplot(viz_df, aes(x = signal_type_factor, y = rank_cor)) +
  geom_hline(yintercept = 0.4, color = "red", linetype = "dashed") +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(data = grey_data, aes(color = outliers), 
              width = 0.2, height = 0, alpha = 0.6, size = 1) +
  geom_jitter(data = blue_data, aes(color = outliers), 
              width = 0.2, height = 0, alpha = 1, size = 1.5) +
  scale_color_manual(values = c("Bottom 3" = "blue", "Top 47" = "grey50")) +
  labs(color = paste0("Rank within\neach signal type\n(", B_global, " draws)")) +
  theme_minimal() +
  labs(
    title = "Rank Stability Across Sample Sizes (Discrete Levels)",
    subtitle = paste0("Each dot = 1 Kendall's τ from 1 draw; ", B_global, " Monte Carlo draws per n/p level",
                      "\nReference level: ", signal_type_global_NL, " with noise proportion ", noise_global_NL),
    x = "Signal Type",
    y = "Kendall's τ",
    caption = paste(
      "Dashed red line indicates τ = 0.4 decision threshold.",
      "\nIf all blue dots are strictly BELOW (and not on) the red line, then rank similarity across datasets is rejected.",
      "\nBlue dots show bottom 3 draws (6% of 50) within each n/p group."
    ))

p_boxplot
```


```{r, dpi=300, fig.width=8, fig.height=5}
p_boxplot +
    labs(
    title = "Rank Stability Across True Signal Type",
    x = "Signal Type",
    y = "Kendall's τ",
    subtitle = paste0("With signal type ", signal_type_global_NL ," as reference group" ), caption = "")
```

# 5. Binomial proportion test

```{r}
# Need to round rank cor to solve floating point issues
sample_prop_name = 'sample prop P(tau >= 0.4)'

z_score_table = viz_df %>% group_by(signal_type) %>% 
  summarise(sample_prop_name = mean(round(rank_cor, 10) >= 0.4)) %>% 
  mutate("sample prop < 0.899" = ifelse(round(sample_prop_name, 10) < 0.899, "NOT similar", "similar"))

names(z_score_table) = c("signal_type", 'sample prop P(tau >= 0.4)', "sample prop < 0.899" )
z_score_table
```


