---
title: "Noise proportion change"
author: "Valentin Kodderitzsch"
date: "2025-04-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Global set up


The goal of this experiment was to investigate the impact of **noise proportion** changes in the German dataset on **rank instability**, using the Dutch dataset as the fixed reference.  

The reference noise proportion is 0.9 (Dutch data), and German noise proportions vary from 0.1 to 0.9 in increments of 0.1.  

The experiment follows a **nested Monte Carlo design**:  
- **Outer loop** (`B_global = 50`): Repeat the full experiment 50 times to account for **sampling variance**.  
- **Inner loop** (`n_noise_global = 9`): Within each repetition, vary the German dataset’s noise proportion, while keeping all other parameters fixed.  


```{r}
set.seed(3895157)

n_global = 10^3 * (4) # sample size
noise_global_NL = 0.9 # noise proportion
noise_global_DE = 0.8 # noise proportion DE

x_factor_global = 1 # A historic artifact that could be removed

noise_prop_list_global = seq(from = 0.1, to =0.9, by = 0.1) 
n_noise_global = length(noise_prop_list_global)

B_global = 50
```

# ##################

# 2. Helper Functions

## Generate Data

True signal: Change signal type by un-commenting.

```{r}
true_signal = function(x1, x2, x3, x4) {
  # atan( (x2*x3 - (1/(x2 * x4))) / x1 ) # Friedman 3
  
  # (x1^2 + (x2*x3 - (1/(x2*x4)) )^2 )^0.5 # Friedman 2
  
  x1 + x2 + x3 +  exp(0.3*x4) * (1 + 0.9 * sin((2*pi/3) * x4)) # Exp + High Freq
  
  # x1 + x2 + x3 +  0.5*x4+1 # G1 linear
  
  # x1 + 3*x2 - 8*x3 + 5 * x4
}
```


Noise proportion

```{r}
# Helper function to compute error variance
get_error_for_signal_prop = function(signal_prop, signal_sd) {
  ((1-signal_prop) / signal_prop ) * signal_sd
}

# True signal variance
f3_signal_var = 0.1002295
```


**FUNCTION EXPLANATION**: `generate_data`  

**Args:**  
1. **noise** (numeric): proportion of variance due to irreducible error.  
2. **x_factor** (numeric): scaling factor applied to predictor ranges (currently has no impact on main logic).  
3. **n** (integer): number of observations.  
4. **use_original_signal_var** (logical): if `TRUE` use `f3_signal_var`, otherwise estimate by simulation.  

**Returns:**  
- **X**: numeric matrix (n × 4) of predictors.  
- **y**: numeric response vector (length n).  
- **signal_var**: numeric; estimated or provided signal variance.  
- **irreducible_error**: numeric; standard deviation of the additive Gaussian noise.  

**Main logic:**  
1. Draw 4 predictors (**x1**…**x4**) from uniform ranges (optionally scaled by `x_factor`).  
2. If `use_original_signal_var = FALSE`:  
   - Simulate `B = 10^3` noiseless responses (`true_signal(...)`) to estimate signal variance.  
   - Otherwise, set `signal_var = f3_signal_var`.  
3. Compute error variance for the requested noise proportion, take square root to obtain **irreducible error** -> raw irreducible error based on `irreducible_error` input 
4. Form `y = true_signal(...) + Normal(0, sd)` 


```{r}
generate_data = function(noise = noise_global, 
                         x_factor = x_factor_global, 
                         n = n_global,
                         use_original_signal_var = T) {
  
  # New ranges so that contribution of x4 towards var(y) is noticeable 
  x1 = runif(n, min = 0, max = 20 * x_factor)
  x2 = runif(n, min = 4 * pi, max = 5.6 * pi * x_factor)
  x3 = runif(n, min = 0, max = 1 * x_factor)
  x4 = runif(n, min = 1, max = 11 * x_factor)
  
  X = data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4)
  X = as.matrix(X)
  
  if (!use_original_signal_var) {
    # Compute true signal variance
    B = 10^3
    signal_list = numeric(B)

    for (idx in 1:B) {
      y_boot = true_signal(x1, x2, x3, x4)
      signal_list[idx] = var(y_boot)
    }
    
    signal_var = mean(signal_list)
    #print(paste("Calculatied signal var instead of using the original -> ", signal_var))
  } else {
      signal_var = f3_signal_var
      #print(paste("Used original signal var -> ", signal_var))
    }
  
  # Compute error variance
  error_var = get_error_for_signal_prop((1-noise), signal_var ) # f3_signal_var
  #print(paste("error var -> ", error_var))

  # Save irreducible error as standard deviation
  irreducible_error = sqrt(error_var)

  # Response
  y = true_signal(x1, x2, x3, x4) + rnorm(n, mean = 0, sd = irreducible_error)
  
  # Return list
  list(X = X, y = y, signal_var = signal_var, irreducible_error = irreducible_error)
}

# Sanity check
df_Dutch = generate_data(x_factor = x_factor_global, use_original_signal_var = F, 
                         noise = noise_global_NL) 
df_German = generate_data(x_factor = 1, use_original_signal_var = F, 
                          noise = noise_global_DE) 
```

### Plot X4

```{r}
get_lm_nrmse = function(input_df) {
  # Reformat input dataframe
  tmp_df = as.data.frame(input_df$X)
  tmp_df$y = input_df$y
  
  # Fit a linear regression model
  lm_fit = lm(y ~ ., data = tmp_df)
  y_hat = predict(lm_fit)
  
  # Compute RMSE
  mse = mean( (tmp_df$y - y_hat)^2 )
  round(sqrt(mse) / sd(tmp_df$y) , 4) # diff(range(tmp_df$y))
}

# X4 component
get_y_for_x4 = function(x4) {
  expr = body(true_signal)[[2]][[3]]
  # print(expr)
  eval(expr, envir = list(x4 = x4))
}

# Set up side-by-side plotting
par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))

# Function to create plot for each dataset
plot_dataset = function(df, country, color) {
  # Get x4 range and create evaluation points
  x4_range = range(df$X[, 4])
  x4_vals = seq(x4_range[1], x4_range[2], length.out = 1000)
  
  # Compute y values and get overall range
  y_pred = sapply(x4_vals, get_y_for_x4) 
  y_range = range(c(df$y, y_pred)) 
  
  # Compute linear NRMSE
  lm_nrmse = get_lm_nrmse(df)
  
  # Create plot
  plot(df$X[, 4], df$y,
       xlab = "x4", ylab = "y",
       main = paste(country, "(", lm_nrmse  ," LM NRMSE)"),
       pch = 16, col = color,
       ylim = y_range)
  
  # Add fitted line
  lines(x4_vals, y_pred, col = "green", lwd = 2) 
}

# Create plots
plot_dataset(df_Dutch, "Dutch", "blue")
plot_dataset(df_German, "German", "red")

# Reset plotting parameters
par(mfrow = c(1, 1))


# Set up side-by-side plotting
par(mfrow = c(3, 4), mar = c(4, 4, 3, 1))

# New code:
for (j in seq_along(noise_prop_list_global)) {
    local_noise = noise_prop_list_global[j]
    
    # Generate data
    df_tmp = generate_data(use_original_signal_var = F, 
                         noise = local_noise)
    plot_dataset(df_tmp, paste("Noise prop", local_noise), "blue")
    
}

# Reset plotting parameters
par(mfrow = c(1, 1))
```


```{r}
# More sanity checks
check_noise_prop = function(df) {
  (df$irreducible_error)^2 / (df$signal_var + (df$irreducible_error)^2 ) 
}

check_noise_prop(df_Dutch)
check_noise_prop(df_German)

var(df_Dutch$y) / 10^1
var(df_German$y) / 10^1
```


```{r}
# Plot to check if the distributions are visibly different
plot(density(df_Dutch$y), main = "Dutch")
plot(density(df_German$y), main = "German")

boxplot(df_Dutch$y, main = "Dutch")
boxplot(df_German$y, main = "German")


# Compute densities
d1 = density(df_Dutch$y)
d2 = density(df_German$y)

# Set axis limits based on both
x_range = range(d1$x, d2$x)
y_max = max(d1$y, d2$y)

# Plot with fixed xlim and ylim
plot(d1, main = "Density of y: Dutch vs German", col = "blue", lwd = 2,
     xlim = x_range, ylim = c(0, y_max))

# Overlay the second density
lines(d2, col = "red", lwd = 2)

# Add legend
legend("topright", legend = c("Dutch", "German"), col = c("blue", "red"), lwd = 2)


```

```{r}
plot(df_German$X[, 1], df_German$y)
plot(df_Dutch$X[, 1], df_Dutch$y)


plot(df_German$X[, 2], df_German$y)
plot(df_Dutch$X[, 2], df_Dutch$y)

plot(df_German$X[, 3], df_German$y)
plot(df_Dutch$X[, 3], df_Dutch$y)

plot(df_German$X[, 4], df_German$y)
plot(df_Dutch$X[, 4], df_Dutch$y)


```


Compute KL and Janson Shanon Divergence scores as reference.

```{r}
kl_divergence_kde = function(p_samples, q_samples, n_points = 1000) {
  # Determine range to cover both distributions
  min_val = min(min(p_samples), min(q_samples))
  max_val = max(max(p_samples), max(q_samples))
  eval_points = seq(min_val, max_val, length.out = n_points)
  
  # Estimate densities using KDE
  p_density = density(p_samples, from = min_val, to = max_val, n = n_points)
  q_density = density(q_samples, from = min_val, to = max_val, n = n_points)
  
  # Extract density values
  p_values = p_density$y
  q_values = q_density$y
  
  # Add small epsilon to avoid division by zero or log(0)
  p_values = p_values + 1e-10
  q_values = q_values + 1e-10
  
  # Normalize to ensure they sum to 1
  p_values = p_values / sum(p_values)
  q_values = q_values / sum(q_values)
  
  # Calculate KL divergence: sum(p * log(p/q))
  kl = sum(p_values * log(p_values / q_values))
  
  return(kl)
}

# Function to calculate Jensen-Shannon divergence using KDE
js_divergence_kde = function(p_samples, q_samples, n_points = 1000) {
  # Determine range to cover both distributions
  min_val = min(min(p_samples), min(q_samples))
  max_val = max(max(p_samples), max(q_samples))
  eval_points = seq(min_val, max_val, length.out = n_points)
  
  # Estimate densities using KDE
  p_density = density(p_samples, from = min_val, to = max_val, n = n_points)
  q_density = density(q_samples, from = min_val, to = max_val, n = n_points)
  
  # Extract density values
  p_values = p_density$y
  q_values = q_density$y
  
  # Add small epsilon to avoid division by zero or log(0)
  p_values = p_values + 1e-10
  q_values = q_values + 1e-10
  
  # Normalize to ensure they sum to 1
  p_values = p_values / sum(p_values)
  q_values = q_values / sum(q_values)
  
  # Calculate the mixture distribution M = (P+Q)/2
  m_values = (p_values + q_values) / 2
  
  # Calculate KL(P||M) and KL(Q||M)
  kl_p_m = sum(p_values * log(p_values / m_values))
  kl_q_m = sum(q_values * log(q_values / m_values))
  
  # JS divergence is the average of the two KL divergences
  js = (kl_p_m + kl_q_m) / 2
  
  return(js)
}

kl = kl_divergence_kde(df_Dutch$y, df_German$y)
js = js_divergence_kde(df_Dutch$y, df_German$y)
print(paste("KL divergence:", kl))
print(paste("JS divergence:", js))
```

KL scores: 
0: Identical
0.1-1: Mild divergence
>1 Divergence 
>5 Large divergence

JSD interpretation:
JSD = 0 -> Distributions are identical.
JSD close to 0 -> Very similar.
JSD ~ 0.5 -> Moderately different.
JSD close to 1 -> Very different.




## Train-Test Algorithms

### Metrics

I evaluated 9 normalized metrics. 6 are well known: MSE, NSE, RSE, RRMSE, NRMSE and MAPE. 3 I developed myself to include a null model: PR, Min-Max and NDRMSE.

```{r}
get_MSE = function(test_Y, test_Y_Hat) {
  mean( (test_Y - test_Y_Hat)^2 )
}

get_NSE = function(test_Y, test_Y_Hat) {
  numerator = sum((test_Y - test_Y_Hat)^2)
  denominator = sum((test_Y - mean(test_Y))^2)
  1 - (numerator / denominator)
}

get_RSE = function(test_Y, test_Y_Hat) {
  numerator = sum((test_Y - test_Y_Hat)^2)
  denominator = sum((test_Y - mean(test_Y))^2)
  numerator / denominator
}

get_RRMSE = function(test_Y, test_Y_Hat) {
  rmse = sqrt(mean((test_Y - test_Y_Hat)^2))
  rmse / mean(test_Y)
}

get_NRMSE = function(test_Y, test_Y_Hat) {
  rmse = sqrt(mean((test_Y - test_Y_Hat)^2))
  rmse / (sd(test_Y) ) #max(test_Y) - min(test_Y)
}

get_MAPE = function(test_Y, test_Y_Hat) {
  if (any(test_Y == 0)) {
    warning("My warning from get_MAP: test_Y contains zeros")
  } else {
    mean(abs( (test_Y - test_Y_Hat)/test_Y )) * 100
  }
}

get_all_6_metrics = function(test_Y, test_Y_Hat) {
  list(
    MSE   = get_MSE(test_Y, test_Y_Hat),
    NSE   = get_NSE(test_Y, test_Y_Hat),
    RSE   = get_RSE(test_Y, test_Y_Hat),
    RRMSE = get_RRMSE(test_Y, test_Y_Hat),
    NRMSE = get_NRMSE(test_Y, test_Y_Hat),
    MAPE  = get_MAPE(test_Y, test_Y_Hat)
  )
}
```

Performance ratio.

```{r}
get_PR = function(MSE_null, MSE_model) {
  sqrt(MSE_null / MSE_model)
}
```


Min-max metric

```{r}
get_min_max = function(MSE_null, MSE_model) {
  # (MSE_null - MSE_model) / MSE_null
  1 - (MSE_model / MSE_null)
}
```

ND-RMSE

```{r}
get_NDRMSE = function(MSE_null, MSE_model, test_Y) {
  numerator =  sqrt(MSE_null) - sqrt(MSE_model) 
  denominator = max(test_Y) - min(test_Y)
  numerator / denominator
  
  # sqrt(MSE_null) / sqrt(MSE_model)
}
```



### Final train-test function

```{r}
# Load all libraries
library(glmnet)
library(randomForest)
library(xgboost)
library("e1071")
library(tidyr)
library(dplyr)
```


**FUNCTION EXPLANATION**: `generate_summary_table`  

**Args:**  
1. **X** (matrix/data frame): predictor variables.  
2. **y** (numeric vector): response variable.  
3. **training_percentage** (numeric): proportion of data used for training (default: 0.75).  
4. **signal_type** (character): passed to `true_signal(...)` to specify the signal type.  

**Returns:**  
- A data frame (7x10) summarizing model performance across 9 metrics (MSE, NSE, RSE, RRMSE, NRMSE, MAPE, PR, MinMax, NDRMSE) for 7 algorithms:  
  - True signal (oracle)  
  - Dummy/null model  
  - Elastic net  
  - Random forest  
  - Linear regression  
  - XGBoost  
  - SVM  

**Main logic:**  
1. Split data into training and test sets.  
2. Define a helper to compute the true signal for all rows in `X` -> Needed by the Oracle 
3. Fit the following models on the training set:  
   - True signal (oracle, no noise)  
   - Null model (mean prediction)  
   - Elastic net (alpha = 0.5)  
   - Random forest  
   - Linear regression  
   - XGBoost  
   - SVM (epsilon regression)  
4. Generate predictions on the test set for each model.  
5. Compute 6 base metrics via `get_all_6_metrics()` for each model.  
6. Calculate additional metrics:  
   - **PR** (performance ratio via `get_PR()`)  
   - **MinMax** (via `get_min_max()`)  
   - **NDRMSE** (via `get_NDRMSE()`)  
7. Combine all results into a summary table.  
8. Reshape data for easier comparison (`pivot_longer` to `pivot_wider`) and sort models by PR in descending order.  
9. Return the final summary data frame.  


```{r}
generate_summary_table = function(X, y, training_percentage = 0.75) {
  # Generate train test split
  train_percentage = training_percentage
  train_size = floor(nrow(X) * train_percentage) 

  train = sample(1:nrow(X), size = train_size)
  test = which(!(1:nrow(X) %in% train))
  
  ##################################################
  
  # Model fitting
  
  ## True signal
  true_signal_all = function(X) {
    y = numeric(nrow(X))
    for (idx in 1:nrow(X)) {
      tmp_row = X[idx, ]
      y[idx] = true_signal(x1 = tmp_row[1], x2 = tmp_row[2], x3 = tmp_row[3], x4 = tmp_row[4])
    }
    return(y)
  }
  
  ## Null model
  null_model = y[train]
  
  ## Elatic net
  # Set alpha = 0.5 for balanced elastic net
  cv_elastic = cv.glmnet(
    x = X[train, ],
    y = y[train],
    alpha = 0.5,
    family = "gaussian",
    nfolds = 5
  )
  
  
  # Fit on entire dataset using optimal lambda value
  model_elastic = glmnet(x = X[train, ],
                         y = y[train],
                         lambda = cv_elastic$lambda.1se)
  
  ## Random forest
  rf = randomForest(x = as.matrix(X[train, ]), y = y[train] )
  
  ## Linear regression
  df_lin = as.data.frame(X) # Create df for lm function
  df_lin$y = y
  
  model_linear = lm(y ~ ., data = df_lin[train, ])
  
  ## XGboost
  # Convert your data to a DMatrix (recommended for xgboost)
  dtrain = xgb.DMatrix(data = as.matrix(X[train, ]), label = y[train])

  # Set parameters
  params = list(
    objective = "reg:squarederror",  # for regression
    lambda = 1,   # L2 regularization (ridge)
    alpha = 1     # L1 regularization (lasso)
  )
  
  # Fit the model with default parameters + regularization
  model_xgb = xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,   # number of boosting rounds (iterations), can tune this
    verbose = 1      # print progress
  )
  
  ## SVM
  model_svm = svm(
    x = X[train, ],    # training predictors
    y = y[train],      # training response
    type = "eps-regression" # regression type (default is epsilon-regression)
  )
  
  
  ##################################################
  
  # Model Evaluation
  
  ## True signal
  # Make prediction
  true_signal_pred = true_signal_all(X[test, ])
  # Evaluate
  metrics.true_signal = get_all_6_metrics(test_Y = y[test], test_Y_Hat = true_signal_pred)
  
  ## Null model
  # Make predictions
  dummy_pred_r2 = rep(mean(y[train]), length(test))
  # Evaluate predictions
  metrics.null_R2 = get_all_6_metrics(test_Y = y[test], test_Y_Hat = dummy_pred_r2)
  
  ## Elatic net
  # Make predictions
  elastic_pred = stats::predict(model_elastic, newx = X[test, ])
  # Evaluate predictions
  metrics.elastic = get_all_6_metrics(test_Y = y[test], test_Y_Hat = elastic_pred)
  
  ## Random forest
  # Make predictions
  rf_pred = stats::predict(rf, newdata = as.matrix(X[test, ] ))
  # Make predictions
  rf_pred = stats::predict(rf, newdata = as.matrix(X[test, ] ))
  # Evaluate predictions
  metrics.rf = get_all_6_metrics(test_Y = y[test], test_Y_Hat = rf_pred)
  
  ## Linear regression
  # Make predictions
  lin_data_X = subset(df_lin, select = -c(y))
  lm_pred = stats::predict(model_linear, newdata = lin_data_X[test, ] )
  # Evaluate predictions
  metrics.linear = get_all_6_metrics(test_Y = y[test], test_Y_Hat = lm_pred)
  
  ## XGboost
  # Make predictions
  xgboost_pred = predict(model_xgb, newdata = as.matrix(X[test, ]))
  # Evaluate predictions
  metrics.xgb = get_all_6_metrics(test_Y = y[test], test_Y_Hat = xgboost_pred)
  
  ## SVM
  # Make predictions
  svm_pred = predict(model_svm, newdata = X[test, ])
  # Evaluate predictions
  metrics.svm = get_all_6_metrics(test_Y = y[test], test_Y_Hat = svm_pred)
  
  ##################################################
  
  # Model Ratios
  
  ## True signal
  ratio.true_signal.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE)  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  ratio.elastic.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE)  
  
  ## Random forest
  ratio.rf.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE) 
  
  ## Linear regression
  ratio.linear.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE)  
  
  ## XGboost
  ratio.xgb.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE)  
  
  ## SVM
  ratio.svm.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE)
  
    ##################################################
  
  # Model Min-Max
  
  ## True signal
  min_max.true_signal.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE)  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  min_max.elastic.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE)  
  
  ## Random forest
  min_max.rf.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE) 
  
  ## Linear regression
  min_max.linear.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE)  
  
  ## XGboost
  min_max.xgb.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE)  
  
  ## SVM
  min_max.svm.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE)  
  
  ##################################################
  
  # ND-RMSE

  ## True signal
  nd_rmse.true_signal.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE,
                              test_Y = y[test])  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  nd_rmse.elastic.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE,
                              test_Y = y[test])  
  
  ## Random forest
  nd_rmse.rf.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE,
                             test_Y = y[test]) 
  
  ## Linear regression
  nd_rmse.linear.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE,
                              test_Y = y[test])  
  
  ## XGboost
  nd_rmse.xgb.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE,
                              test_Y = y[test])  
  
  ## SVM
  nd_rmse.svm.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE,
                              test_Y = y[test])  

  
  ##################################################
  
  # Summary
  df_summary = data.frame(
    Metric = c(names(metrics.true_signal), "PR", "MinMax", "NDRMSE"),
    True_signal = c(unname(unlist(metrics.true_signal)), ratio.true_signal.R2, min_max.true_signal.R2, nd_rmse.true_signal.R2),
    Dummy_R2 = c(unname(unlist(metrics.null_R2)), NA, NA, NA),
    elastic_net = c(unname(unlist(metrics.elastic)), ratio.elastic.R2, min_max.elastic.R2, nd_rmse.elastic.R2),
    RF = c(unname(unlist(metrics.rf)), ratio.rf.R2, min_max.rf.R2, nd_rmse.rf.R2),
    Linear_reg = c(unname(unlist(metrics.linear)), ratio.linear.R2, min_max.linear.R2, nd_rmse.linear.R2),
    XGB = c(unname(unlist(metrics.xgb)), ratio.xgb.R2, min_max.xgb.R2, nd_rmse.xgb.R2),
    SVM = c(unname(unlist(metrics.svm)), ratio.svm.R2, min_max.svm.R2, nd_rmse.svm.R2)
  )
  
  # Pivot all columns except for the "metric" column into a new column called "model"
  # The values go into the values column
  df_long = df_summary %>%
    pivot_longer(
      cols = -Metric,
      names_to = "model",
      values_to = "value")

  # Pivot each metric name from the metric column into its own column
  df_wide = df_long %>%
    pivot_wider(
      names_from = Metric,
      values_from = value)
  
  df_final = df_wide %>%
    arrange(desc(PR))
  
  ##################################################
  
  # # Memory clean up
  # objects_to_keep = "df_final"
  # all_objects = ls() # List all environments inside this function
  # to_remove = setdiff(all_objects, objects_to_keep)
  # #print(to_remove)
  # rm(list = to_remove) # Remove
  # 
  # # Force garbage collection
  # gc(verbose = F)
  # invisible(gc(full = TRUE, verbose = F))
  
  # Final function output
  return(df_final)
}


# Sanity check
df_Dutch_summary = generate_summary_table(X = df_Dutch$X, y = df_Dutch$y)
df_Dutch_summary

df_German_summary = generate_summary_table(X = df_German$X, y = df_German$y)
df_German_summary
```


```{r}
(df_German$irreducible_error)^2

# Reducible error (raw values) -> True signal
df_German_summary[df_German_summary$model == "True_signal" , "MSE"] - (df_German$irreducible_error)^2

# Reducible error (raw values) -> All models
df_German_summary[, "MSE"] - (df_German$irreducible_error)^2

#
(df_German_summary[, "MSE"] - (df_German$irreducible_error)^2 ) / df_German_summary[, "MSE"] * 100

(df_German$irreducible_error)^2 / df_German_summary[, "MSE"] 
```



## Rank Correlation

**FUNCTION EXPLANATION**: `get_rank_cor`  

**Args:**  
1. **df_Dutch** (data frame): performance metrics for models evaluated on the Dutch dataset -> Reference.  
2. **df_German** (data frame): performance metrics for models evaluated on the German dataset -> Non-reference.  
3. **exclude_model** (character vector): models to exclude from ranking comparison (default: `c("True_signal", "Dummy_R2")`).  
4. **metrics_list** (character vector): list of metrics to evaluate rank correlation for (default: `c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE")`).  
5. **higher_is_better_list** (named logical vector): for each metric, `TRUE` if higher values indicate better performance, `FALSE` otherwise. Defaults:  
   - NSE = TRUE, RSE = FALSE, RRMSE = FALSE, NRMSE = FALSE,  
     MAPE = FALSE, PR = TRUE, MSE = FALSE, MinMax = TRUE, NDRMSE = TRUE.  

**Returns:**  
- A named numeric vector of Kendall rank correlations between the “true” ranking (based on Dutch MSE) and the proposed rankings from the German dataset, for each metric in `metrics_list`.  

**Main logic:**  
1. Remove excluded models from the Dutch dataset.  
2. Determine the **true ranking** of models using Dutch data, sorting by lowest MSE.  
   - Assign **fixed reference IDs** \(1, 2, \dots, n\) to these models (each ID corresponds to a specific model).  
3. Remove excluded models from the German dataset.  
4. For each metric in `metrics_list`:  
   - Sort German models by the metric, in ascending or descending order depending on `higher_is_better_list`.  
   - Convert the ordered German model names into **IDs** using the Dutch reference mapping.  
   - These IDs represent the **German ranking in the Dutch ID system**.  
   - Compute Kendall rank correlation between the Dutch fixed IDs and the mapped German IDs.  
5. Return a vector of correlations, named by metric.  

**Example of mapping logic:**  
- Dutch ranking: `["ModelA", "ModelB", "ModelC"]` -> ranks: `1, 2, 3`  
- German ranking: `["ModelC", "ModelA", "ModelB"]` -> Mapped German rankings will be `3, 1, 2`  
- Kendall correlation is computed between `1, 2, 3` and `3, 1, 2`. 

```{r}
get_rank_cor = function(df_Dutch, df_German, exclude_model = c("True_signal", "Dummy_R2"),
                        metrics_list = c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE"),
                        higher_is_better_list = c(NSE = TRUE, RSE = FALSE, RRMSE = FALSE, NRMSE = FALSE, 
                                                       MAPE = FALSE, PR = TRUE, MSE = FALSE, MinMax = TRUE, NDRMSE = TRUE)) {
  # Step 1: True ranking -> Based on dutch$MSE
  df_Dutch_5_models = df_Dutch %>% filter(! model %in% exclude_model)
  
  true_ranking_names = df_Dutch_5_models[order(df_Dutch_5_models$MSE), ] %>% pull(model)
  true_ranking_idx = 1:length(true_ranking_names)
  
  print("True ranking list")
  print(true_ranking_names)
  
  # Step 2: Proposed ranking
  df_German_5_models = df_German %>% filter(! model %in% exclude_model)
  
  # Initialize results vector
  results = vector("numeric", length = length(metrics_list))
  names(results) = metrics_list
  
  # Compute rank correlation for each metric
  for (metric in metrics_list) {
    proposed_ranking_names = df_German_5_models[order(df_German_5_models[[metric]], decreasing = higher_is_better_list[metric]), ] %>% 
      pull(model)
    
    proposed_ranking_idx = match(true_ranking_names, proposed_ranking_names)
    
    print(paste("Proposed ranking for -> ", metric))
    print(proposed_ranking_names)
    print(proposed_ranking_idx)
    
    # Compute rank correlation
    results[metric] = cor(true_ranking_idx, proposed_ranking_idx, method = "kendall")
  }
  
  return(results)
  
  # Compute for MSE for now -> Loop post sanity check
  # proposed_ranking_names = df_German_5_models[order(df_German_5_models$MSE), ] %>% pull(model)
  # proposed_ranking_idx = match(true_ranking_names, proposed_ranking_names)
  # 
  # 
  # 
  # rank_cor = cor(true_ranking_idx, proposed_ranking_idx, method = "kendall")
  # print(rank_cor)
}

# XGB, RF, SVM, Linear, E net <= Dutch
# RF, Linear, SVM, E net, XGB <= German
get_rank_cor(df_Dutch_summary, df_German_summary, metrics_list = c("MSE"), higher_is_better_list = c("MSE"=F))
```


# #######################

# 3. Main Simulation Loop

Progress bar
```{r}
library(progress)
```


**CODE EXPLANATION**: Nested simulation over noise proportions  

**Purpose:**  
Repeat the experiment **`B_global` times** to account for **sampling variance**, while systematically varying only the **noise proportion** for the German dataset (non-reference).  

- **Outer loop**: Represents one full replication (i.e. Monte Carlo run) of the experiment -> Dutch dataset (reference) is regenerated but with fixed noise level.  
- **Inner loop**: Iterates over the list of noise proportions (`noise_prop_list_global`) for the German dataset, keeping all other parameters constant.  


```{r}
# Timing the code
start.time = Sys.time()

# Initialize an empty dataframe to store all results
all_results_df = data.frame()
sim_model_results_df = data.frame()
rank_cor_sim_df = data.frame()

# Progress bar
pb = progress_bar$new(format = "(:spin) [:bar] :percent [Elapsed time: :elapsedfull || Estimated time remaining: :eta]",
                       total = B_global * n_noise_global,
                       complete = "=",   # Completion bar character
                       incomplete = "-", # Incomplete bar character
                       current = ">",    # Current bar character
                       clear = FALSE,    # If TRUE, clears the bar when finish
                       width = 100)      # Width of the progress bar

for (idx in 1:B_global) {
  # Reference dataset stays fixed
  df_Dutch = generate_data(x_factor = x_factor_global, use_original_signal_var = FALSE, noise = noise_global_NL)
  df_Dutch_summary = generate_summary_table(X = df_Dutch$X, y = df_Dutch$y)
    
  for (j in seq_along(noise_prop_list_global)) {
    local_noise = noise_prop_list_global[j]

    # Generate datasets
    df_German = generate_data(x_factor = 1, use_original_signal_var = FALSE, noise = local_noise)

    # Generate summary tables
    df_German_summary = generate_summary_table(X = df_German$X, y = df_German$y)
    
    # Model performance over noise  ###########
    # Convert to longer format
    df_DE_longer = df_German_summary %>% 
      pivot_longer(cols = -model, names_to = "metric", values_to = "score")
    df_DE_longer$noise = local_noise
    df_DE_longer$simulation = idx
    sim_model_results_df = rbind(sim_model_results_df, df_DE_longer)

    # Compute RD metrics   ###########
    RD_df = calculate_all_RD(df_Dutch = df_Dutch_summary, df_German = df_German_summary)
    
    # Add noise and simulation iteration columns
    RD_df$noise = local_noise
    RD_df$simulation = idx
    
    # Append to the main results dataframe
    all_results_df = rbind(all_results_df, RD_df)
    
    # Compute rank correlation   ###########
    rank_cor = get_rank_cor(df_Dutch_summary, df_German_summary)
    rank_cor_df = data.frame(metric = names(rank_cor), rank_cor = unname(rank_cor))
    rank_cor_df$noise = local_noise
    rank_cor_df$simulation = idx
    
    rank_cor_sim_df = rbind(rank_cor_sim_df,rank_cor_df )
    
    pb$tick()
  }
}
end.time = Sys.time()
time.taken = end.time - start.time
time.taken


# Show results
all_results_df
```

```{r}
# Step 1: Create a timestamp
timestamp = format(Sys.time(), "%Y-%m-%d_%H-%M-%S")

filename_ranks = paste0(
  "ranks_",
  timestamp,
  "_refNoise=", noise_global_NL,
  "_B=", B_global,
  ".Rdata"
)

filename_metric_scores = paste0(
  "metric-scores_",
  timestamp,
  "_refNoise=", noise_global_NL,
  "_B=", B_global,
  ".Rdata"
)

# Save files
save(rank_cor_sim_df, file = filename_ranks)
save(all_results_df, file = filename_metric_scores)
```


# #######################

# 4. Visualization


```{r}
library(ggplot2)
library(dplyr)
library(scales)
library(ggrepel)
```


## Individual algorithms

```{r}
# Create a vector of all possible models
all_possible_models = df_German_summary %>% pull(model)

# Get default ggplot colors for all metrics
model_colors = setNames(hue_pal()(length(all_possible_models)), all_possible_models)
model_colors
```


```{r}
model_labels = c("True_signal" = "True Function",
                 "Linear_reg" = "Linear Regression",
                 "SVM" = "SVM",
                 "RF" = "Random Forest",
                 "elastic_net" = "Elastic Net",
                 "XGB" = "XGBoost",
                 "Dummy_R2" = "Null Model")
```


```{r}
sim_1 = sim_model_results_df %>% 
  filter(simulation == 1)

ggplot(data = sim_1, mapping = aes(x = noise, y = score, color = model)) +
  geom_line(na.rm = T) +
  facet_wrap(~ metric, scales = "free_y") +
  scale_color_manual(values = model_colors,
                     labels = model_labels,
                     name = "My Legend I am Legend")

# NO DUMMY  
sim_1_no_dummy = sim_model_results_df %>% 
  filter(simulation == 1, model != "Dummy_R2")

ggplot(data = sim_1_no_dummy, mapping = aes(x = noise, y = score, color = model)) +
  geom_line(na.rm = T) +
  facet_wrap(~ metric, scales = "free_y") +
  labs(title = "Sim 1")

# ONLY RF, Linear, True
sim_1_rf_lin_true = sim_model_results_df %>% 
  filter(simulation == 1, model %in% c("Linear_reg")) # , "RF", "True_signal"

ggplot(data = sim_1_rf_lin_true, mapping = aes(x = noise, y = score, color = model)) +
  geom_line(na.rm = T) +
  facet_wrap(~ metric, scales = "free_y") +
  scale_color_manual(values = model_colors)
```

Plot using SD as well

```{r}
# Summarize: mean and sd of score over simulations
summary_df = sim_model_results_df %>%
  filter(model %in% c("Linear_reg", "RF", "True_signal")) %>%
  group_by(model, metric, noise) %>%
  summarise(
    mean_score = mean(score, na.rm = TRUE),
    sd_score = sd(score, na.rm = TRUE),
    .groups = "drop"
  )

selected_model_labels = c("True_signal" = "True Function",
                 "Linear_reg" = "Linear Regression",
                 "RF" = "Random Forest")

# Plot: with ribbon for ± SD
ggplot(summary_df, aes(x = noise, y = mean_score, color = model, fill = model)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score), 
              alpha = 0.2, color = NA, show.legend = F) +
  facet_wrap(~ metric, scales = "free_y") +
    labs(
    title = "Model Performance Over Noise Proportion",
    subtitle = paste("Mean score ± 1 SD over", B_global, "simulations",
                     "\nTrue function:", paste(deparse(body(true_signal)[[2]] ), collapse = " ")),
    x = "Noise Proportion",
    y = "Score",
    color = "Model",
    fill = "Model"
  ) +
  scale_x_continuous(
    breaks = seq(0, 1, by = 0.2)
  ) +
  scale_color_discrete(labels = selected_model_labels,
                     name = "Selected Models") +
  theme_minimal()

```
```{r}
# Summarize: mean and sd of score over simulations
summary_df = sim_model_results_df %>%
  # filter(model %in% c("Linear_reg", "RF", "True_signal")) %>%
  group_by(model, metric, noise) %>%
  summarise(
    mean_score = mean(score, na.rm = TRUE),
    sd_score = sd(score, na.rm = TRUE),
    .groups = "drop"
  )

selected_model_labels = c("True_signal" = "True Function", 
                          "Linear_reg" = "Linear Regression",
                          "SVM" = "SVM",
                          "RF" = "Random Forest",
                          "elastic_net" = "Elastic Net",
                          "XGB" = "XGBoost",
                          "Dummy_R2" = "Null Model")

# Plot: with ribbon for ± SD
ggplot(summary_df, aes(x = noise, y = mean_score, color = model, fill = model)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score), 
              alpha = 0.2, color = NA, show.legend = F) +
  facet_wrap(~ metric, scales = "free_y") +
    labs(
    title = "Model Performance Over Noise Proportion",
    subtitle = paste("Mean score ± 1 SD over", B_global, "simulations",
                     "\nTrue function:", paste(deparse(body(true_signal)[[2]] ), collapse = " ") ),
    x = "Noise Proportion",
    y = "Score",
    color = "Model",
    fill = "Model"
  ) +
  scale_x_continuous(
    breaks = seq(0, 1, by = 0.2)
  ) +
  scale_color_discrete(labels = selected_model_labels,
                     name = "Selected Models") +
  theme_minimal()
```

### Thesis plots

```{r}
summary_df = sim_model_results_df %>%
  filter(model %in% c("Linear_reg", "XGB", "True_signal")) %>%
  group_by(model, metric, noise) %>%
  summarise(
    mean_score = mean(score, na.rm = TRUE),
    sd_score = sd(score, na.rm = TRUE),
    .groups = "drop"
  )

# Fix legend
selected_model_labels = c("True_signal" = "Oracle",
                 "Linear_reg" = "Linear Regression",
                 "XGB" = "XGBoost")


algo_plot = ggplot(summary_df %>% filter(metric %in% c("MSE", "NSE", "NRMSE", "RRMSE", "PR")), 
       aes(x = noise, y = mean_score, color = model, fill = model, group = model)) +
  geom_line() +
  geom_point(show.legend = F, size = 0.6) +
  geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score), 
              alpha = 0.2, color = NA, show.legend = F) +
  facet_wrap(~ metric, scales = "free_y") +
  labs(
    title = "Performance Across Noise Levels and Algorithms",
    subtitle = paste0("Mean performance ± 1 SD over ", B_global, " Monte Calro simulations"),
    x = "Noise Proportion \nVar(error)/Var(Y)",
    y = "Metric Performance Score",
    color = "Model",
    fill = "Model"
  ) +
  scale_color_discrete(labels = selected_model_labels,
                     name = "Selected Algorithm") +
  theme_minimal()

algo_plot
```

```{r, dpi=300, fig.width=8, fig.height=5}
algo_plot
```

## Ranks

```{r}
sim_1 = rank_cor_sim_df %>% 
  filter(simulation == 1)

ggplot(data = sim_1, mapping = aes(x = noise, y = rank_cor, color = metric)) +
  geom_point(na.rm = T) +
  facet_wrap(~ metric, scales = "free_y")
```

```{r}
# Summarize: mean and sd of ranks correlation over simulations
summary_df = rank_cor_sim_df %>%
  group_by(metric, noise) %>%
  summarise(
    mean_rank_cor = mean(rank_cor, na.rm = TRUE),
    sd_rank_cor = sd(rank_cor, na.rm = TRUE),
    q5_rank = quantile(rank_cor, 0.05),
    q95_rank = quantile(rank_cor, 0.95),
    .groups = "drop"
  )

summary_df

# Plot: with ribbon for ± SD
ggplot(summary_df, aes(x = noise, y = mean_rank_cor, color = metric, fill = metric)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean_rank_cor - sd_rank_cor,
                  ymax = mean_rank_cor + sd_rank_cor),
              width = 0.05) +
  geom_ribbon(aes(ymin = mean_rank_cor - sd_rank_cor, ymax = mean_rank_cor + sd_rank_cor), 
              alpha = 0.2, color = NA) +
  facet_wrap(~ metric) +
    labs(
    title = "Rank Correlation Across Noise Proportions",
    subtitle = paste("Mean rank correlation ± 1 SD over", B_global, "simulations",
                     "\nReference noise level is ", noise_global_NL,
                     "\nTrue function:", paste(deparse(body(true_signal)[[2]] ), collapse = " ")),
    x = "Noise Proportion",
    y = "Rank Correlation",
    color = "Model",
    fill = "Model"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(-1.2, 1.2))
```

```{r}
summary_df %>% 
  filter(metric == "MSE")
```


### Thesis plot

```{r}
viz_df = rank_cor_sim_df %>% filter(metric == "MSE")

# Order by tau (increasing) within each group
viz_df = viz_df %>% group_by(noise) %>% 
  arrange(noise, rank_cor) %>% 
  mutate(id_per_group = row_number())

# Color the bottom 3 draws (6% of 50) within each group blue, the rest grey
n_outliers = ceiling(0.05 * B_global)  # Bottom 3 draws = 6% of 50 draws
viz_df = viz_df %>% 
  mutate(outliers = ifelse(id_per_group %in% 1:n_outliers, "Bottom 3", "Top 47"))

# Convert sample_size_per_cov to factor for discrete x-axis
viz_df$noise_factor = factor(viz_df$noise, 
                                           levels = sort(unique(viz_df$noise)))

# Filter data for each layer
grey_data = viz_df %>% filter(outliers == "Top 47")
blue_data = viz_df %>% filter(outliers == "Bottom 3")

# Create boxplot version
p_boxplot = ggplot(viz_df, aes(x = noise_factor, y = rank_cor)) +
  geom_hline(yintercept = 0.4, color = "red", linetype = "dashed") +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(data = grey_data, aes(color = outliers), 
              width = 0.2, height = 0, alpha = 0.6, size = 1) +
  geom_jitter(data = blue_data, aes(color = outliers), 
              width = 0.2, height = 0, alpha = 1, size = 1.5) +
  scale_color_manual(values = c("Bottom 3" = "blue", "Top 47" = "grey50")) +
  labs(color = paste0("Rank within\neach noise level\n(", B_global, " draws)")) +
  theme_minimal() +
  labs(
    title = "Rank Stability Across Noise Levels",
    subtitle = paste0("Each dot = 1 Kendall's τ from 1 draw; ", B_global, " Monte Carlo draws per n/p level",
                      "\nReference noise level: ", noise_global_NL,
                      "\nTrue function:", paste(deparse(body(true_signal)[[2]] )) ),
    x = "Noise Proportion \nVar(error)/Var(Y)",
    y = "Kendall's τ",
    caption = paste(
      "Dashed red line indicates τ = 0.4 decision threshold.",
      "\nIf all blue dots are strictly BELOW (and not on) the red line, then rank similarity across datasets is rejected.",
      "\nBlue dots show bottom 3 draws (6% of 50) within each n/p group."
    ))

p_boxplot
```

```{r, dpi=300, fig.width=8, fig.height=5}
p_boxplot +
  labs(subtitle = paste0("With noise level ", noise_global_NL, " as reference level"),
       caption = "")
```


# 5. Binomial proportion test

```{r}
# Need to round rank cor to solve floating point issues
sample_prop_name = 'sample prop P(tau >= 0.4)'

z_score_table = viz_df %>% group_by(noise) %>% 
  summarise(sample_prop_name = mean(round(rank_cor, 10) >= 0.4)) %>% 
  mutate("sample prop < 0.899" = ifelse(round(sample_prop_name, 10) < 0.899, "NOT similar", "similar"))

names(z_score_table) = c("noise", 'sample prop P(tau >= 0.4)', "sample prop < 0.899" )
z_score_table
```

```{r}
viz_df %>% filter(noise == 0.9) %>% arrange(rank_cor)
```

