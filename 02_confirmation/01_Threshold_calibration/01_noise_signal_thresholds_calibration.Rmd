---
title: "Threshold calibration"
author: "Valentin Kodderitzsch"
date: "2025-06-09"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Gloabl set up

The goal of this experiment was to empirically **calibrate the framework's thresholds** by systematically exploring the entire problem space defined by noise and signal type.

The Dutch dataset serves as the **fixed reference point**, set at an intermediate noise level (0.3) and moderately high signal complexity (type 4). The German dataset is the **testing dataset**, systematically varied across all 45 combinations of noise proportion (9 levels) and signal type (5 levels).

The experiment follows a **nested Monte Carlo design**:
- **Outer loop** (`B_global = 50`): Repeat the full experiment 50 times to account for **sampling variance** in the estimated curvature scores and rank correlations.
- **Nested inner loops** (`n_noise_global = 9` and `n_signal_global = 5`): Within each repetition, iterate through all noise and signal combinations for the German dataset.

```{r}
set.seed(3895157)

n_global = 10^3 * (4) # sample size
B_global = 50 # Samples per subject-treatment level -> B=1 takes about 2 min


# Reference point  #######################
noise_global_NL = 0.3
signal_type_global_NL = 4
# ####################### ################

# Example moving dataset
noise_global_DE = 0.6
signal_type_global_DE = 5

# Moving dataset parameters #######################
# Noise
noise_prop_list_global = round(seq(from = 0.1, to =0.9, by = 0.1), 10) # Avoid floating point problems
n_noise_global = length(noise_prop_list_global)

# Signal type
n_signal_global = 5 # 4
#  ####################### #######################


# Claim box threshold
claim_box_noise_global = 0.2
claim_box_LM_global = 0.15
```


# 2. Helper functions

## True signal

```{r}
true_signal = function(x1, x2, x3, x4, type) {
  if (type > n_signal_global ) {
    stop("Type > 4 not possible since only 4 true signals present")
  } else {
      switch(type,
         # Case 1
         x1 + x2 + x3 +  (0.5*x4+1),
         # Case 2
         x1 + x2 + x3 +  0.15*x4^2,
         # Case 3
         x1 + x2 + x3 +  0.15*x4^3,
         # Case 4
         x1 + x2 + x3 +  exp(0.3*x4) * (1 + 0.6 * sin((2*pi/10) * x4)) ,
         # Case 5
         x1 + x2 + x3 +  exp(0.3*x4) * (1 + 0.9 * sin((2*pi/3) * x4)) # 3 instead of 0.3
         
         # # Case 1
         # x1 + x2 + x3 +  10*sin((2*pi/9) * x4) +5, # 20
         # # Case 2
         # x1 + x2 + x3 +  10*sin((2*pi/5) * x4) +5, # 10
         # # Case 3
         # x1 + x2 + x3 +  10*sin((2*pi/3) * x4) +5 , # 5
         # # Case 4
         # x1 + x2 + x3 +  10*sin((2*pi/2) * x4) +5 # 2 instead of 0.2
         ) 
  }
}
```


## Generate data

**FUNCTION EXPLANATION**: `generate_data`  

**Args:**  
1. **noise** (numeric): proportion of variance due to irreducible error.  
2. **n** (integer): number of observations.  
3. **use_original_signal_var** (logical): if `TRUE` use `f3_signal_var`, otherwise estimate by simulation.  
4. **type** (character): passed to `true_signal(...)` to select the signal form.  

**Returns:**  
- **X**: numeric matrix (n × 4) of predictors  
- **y**: numeric response vector (length n)  
- **signal_var**: numeric; estimated or provided signal variance  
- **irreducible_error**: numeric; standard deviation of the additive Gaussian noise  

**Main logic:**  
1. draw 4 predictors from their uniform ranges (x1..x4)  
2. If `use_original_signal_var = FALSE`:  
   - Simulate `B = 10^3` noiseless responses (`true_signal(...)`) to estimate signal variance.  
   - Otherwise, set `signal_var = f3_signal_var`.  
3. Compute error variance for the requested noise proportion, take square root to obtain **irreducible error** -> raw irreducible error based on `irreducible_error` input 
4. Form `y = true_signal(...) + Normal(0, sd)` 

```{r}
# Helper function to compute error variance
get_error_for_signal_prop = function(signal_prop, signal_sd) {
  ((1-signal_prop) / signal_prop ) * signal_sd
}

generate_data = function(noise = noise_global, 
                         n = n_global,
                         use_original_signal_var = T,
                         type) {
  # Explanatory variables
  x1 = runif(n, min = 0, max = 20 )
  x2 = runif(n, min = 4 * pi, max = 5.6 * pi )
  x3 = runif(n, min = 0, max = 1 )
  x4 = runif(n, min = 1, max = 11 )
  
  X = data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4)
  X = as.matrix(X)
  
  if (!use_original_signal_var) {
    # Compute true signal variance
    B = 10^3
    signal_list = numeric(B)

    for (idx in 1:B) {
      y_boot = true_signal(x1, x2, x3, x4, type)
      signal_list[idx] = var(y_boot)
    }
    
    signal_var = mean(signal_list)
    #print(paste("Calculatied signal var instead of using the original -> ", signal_var))
  } else {
      signal_var = f3_signal_var
      #print(paste("Used original signal var -> ", signal_var))
    }
  
  # Compute error variance
  error_var = get_error_for_signal_prop((1-noise), signal_var ) # f3_signal_var
  #print(paste("error var -> ", error_var))

  # Save irreducible error as standard deviation
  irreducible_error = sqrt(error_var)

  # Response
  y = true_signal(x1, x2, x3, x4, type) + rnorm(n, mean = 0, sd = irreducible_error)
  
  # Return list
  list(X = X, y = y, signal_var = signal_var, irreducible_error = irreducible_error)
}

# Sanity check
df_Dutch = generate_data( use_original_signal_var = F, 
                         noise = noise_global_NL, type = signal_type_global_NL)
df_German = generate_data(use_original_signal_var = F, 
                          noise = noise_global_DE, type = signal_type_global_DE)
```


## Train-Test Algorithms

```{r}
get_NSE_and_LM_from_summary = function(df) {
  # nse = df[order( df[["NSE"]])] %>% select(model) %>% top_n(1) %>% pull()
  nse = max(df[["NSE"]])
  
  lm = df %>% filter(model == "Linear_reg") %>% pull(NRMSE)

  output = list(nse = nse, lm = lm)
  return(output)
}

get_LM = function(input_df) {
  # Reformat input dataframe
  tmp_df = as.data.frame(input_df$X)
  tmp_df$y = input_df$y
  
  # Fit a linear regression model
  lm_fit = lm(y ~ ., data = tmp_df)
  y_hat = predict(lm_fit)
  
  # Compute RMSE
  mse = mean( (tmp_df$y - y_hat)^2 )
  round(sqrt(mse) / sd(tmp_df$y) , 4) # diff(range(tmp_df$y))
}
```


I evaluated 9 normalized metrics. 6 are well known: MSE, NSE, RSE, RRMSE, NRMSE and MAPE. 3 I developed myself to include a null model: PR, Min-Max and NDRMSE.

```{r}
get_MSE = function(test_Y, test_Y_Hat) {
  mean( (test_Y - test_Y_Hat)^2 )
}

get_NSE = function(test_Y, test_Y_Hat) {
  numerator = sum((test_Y - test_Y_Hat)^2)
  denominator = sum((test_Y - mean(test_Y))^2)
  1 - (numerator / denominator)
}

get_RSE = function(test_Y, test_Y_Hat) {
  numerator = sum((test_Y - test_Y_Hat)^2)
  denominator = sum((test_Y - mean(test_Y))^2)
  numerator / denominator
}

get_RRMSE = function(test_Y, test_Y_Hat) {
  rmse = sqrt(mean((test_Y - test_Y_Hat)^2))
  rmse / mean(test_Y)
}

get_NRMSE = function(test_Y, test_Y_Hat) {
  rmse = sqrt(mean((test_Y - test_Y_Hat)^2))
  rmse / ( sd(test_Y) ) #max(test_Y) - min(test_Y)
}

get_MAPE = function(test_Y, test_Y_Hat) {
  if (any(test_Y == 0)) {
    warning("My warning from get_MAP: test_Y contains zeros")
  } else {
    mean(abs( (test_Y - test_Y_Hat)/test_Y )) * 100
  }
}

get_all_6_metrics = function(test_Y, test_Y_Hat) {
  list(
    MSE   = get_MSE(test_Y, test_Y_Hat),
    NSE   = get_NSE(test_Y, test_Y_Hat),
    RSE   = get_RSE(test_Y, test_Y_Hat),
    RRMSE = get_RRMSE(test_Y, test_Y_Hat),
    NRMSE = get_NRMSE(test_Y, test_Y_Hat),
    MAPE  = get_MAPE(test_Y, test_Y_Hat)
  )
}
```


```{r}
get_PR = function(MSE_null, MSE_model) {
  sqrt(MSE_null / MSE_model)
}

get_min_max = function(MSE_null, MSE_model) {
  # (MSE_null - MSE_model) / MSE_null
  1 - (MSE_model / MSE_null)
}

get_NDRMSE = function(MSE_null, MSE_model, test_Y) {
  numerator =  sqrt(MSE_null) - sqrt(MSE_model) 
  denominator = max(test_Y) - min(test_Y)
  numerator / denominator
  
  # sqrt(MSE_null) / sqrt(MSE_model)
}
```

```{r}
# Load all libraries
library(glmnet)
library(randomForest)
library(xgboost)
library("e1071")
library(tidyr)
library(dplyr)
```



**FUNCTION EXPLANATION**: `generate_summary_table`  

**Args:**  
1. **X** (matrix/data frame): predictor variables.  
2. **y** (numeric vector): response variable.  
3. **training_percentage** (numeric): proportion of data used for training (default: 0.75).  
4. **signal_type** (character): passed to `true_signal(...)` to specify the signal type.  

**Returns:**  
- A data frame (7x10) summarizing model performance across 9 metrics (MSE, NSE, RSE, RRMSE, NRMSE, MAPE, PR, MinMax, NDRMSE) for 7 algorithms:  
  - True signal (oracle)  
  - Dummy/null model  
  - Elastic net  
  - Random forest  
  - Linear regression  
  - XGBoost  
  - SVM  

**Main logic:**  
1. Split data into training and test sets.  
2. Define a helper to compute the true signal for all rows in `X` -> Needed by the Oracle 
3. Fit the following models on the training set:  
   - True signal (oracle, no noise)  
   - Null model (mean prediction)  
   - Elastic net (alpha = 0.5)  
   - Random forest  
   - Linear regression  
   - XGBoost  
   - SVM (epsilon regression)  
4. Generate predictions on the test set for each model.  
5. Compute 6 base metrics via `get_all_6_metrics()` for each model.  
6. Calculate additional metrics:  
   - **PR** (performance ratio via `get_PR()`)  
   - **MinMax** (via `get_min_max()`)  
   - **NDRMSE** (via `get_NDRMSE()`)  
7. Combine all results into a summary table.  
8. Reshape data for easier comparison (`pivot_longer` to `pivot_wider`) and sort models by PR in descending order.  
9. Return the final summary data frame.  


```{r}
generate_summary_table = function(X, y, training_percentage = 0.75, signal_type) {
  # Generate train test split
  train_percentage = training_percentage
  train_size = floor(nrow(X) * train_percentage) 

  train = sample(1:nrow(X), size = train_size)
  test = which(!(1:nrow(X) %in% train))
  
  ##################################################
  
  # Model fitting
  
  ## True signal
  true_signal_all = function(X) {
    y = numeric(nrow(X))
    for (idx in 1:nrow(X)) {
      tmp_row = X[idx, ]
      y[idx] = true_signal(x1 = tmp_row[1], x2 = tmp_row[2], x3 = tmp_row[3], x4 = tmp_row[4], type = signal_type)
    }
    return(y)
  }
  
  ## Null model
  null_model = y[train]
  
  ## Elatic net
  # Set alpha = 0.5 for balanced elastic net
  cv_elastic = cv.glmnet(
    x = X[train, ],
    y = y[train],
    alpha = 0.5,
    family = "gaussian",
    nfolds = 5
  )
  
  
  # Fit on entire dataset using optimal lambda value
  model_elastic = glmnet(x = X[train, ],
                         y = y[train],
                         lambda = cv_elastic$lambda.1se)
  
  ## Random forest
  rf = randomForest(x = as.matrix(X[train, ]), y = y[train] )
  
  ## Linear regression
  df_lin = as.data.frame(X) # Create df for lm function
  df_lin$y = y
  
  model_linear = lm(y ~ ., data = df_lin[train, ])
  
  ## XGboost
  # Convert your data to a DMatrix (recommended for xgboost)
  dtrain = xgb.DMatrix(data = as.matrix(X[train, ]), label = y[train])

  # Set parameters
  params = list(
    objective = "reg:squarederror",  # for regression
    lambda = 1,   # L2 regularization (ridge)
    alpha = 1     # L1 regularization (lasso)
  )
  
  # Fit the model with default parameters + regularization
  model_xgb = xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,   # number of boosting rounds (iterations), can tune this
    verbose = 1      # print progress
  )
  
  ## SVM
  model_svm = svm(
    x = X[train, ],    # training predictors
    y = y[train],      # training response
    type = "eps-regression" # regression type (default is epsilon-regression)
  )
  
  
  ##################################################
  
  # Model Evaluation
  
  ## True signal
  # Make prediction
  true_signal_pred = true_signal_all(X[test, ])
  # Evaluate
  metrics.true_signal = get_all_6_metrics(test_Y = y[test], test_Y_Hat = true_signal_pred)
  
  ## Null model
  # Make predictions
  dummy_pred_r2 = rep(mean(y[train]), length(test))
  # Evaluate predictions
  metrics.null_R2 = get_all_6_metrics(test_Y = y[test], test_Y_Hat = dummy_pred_r2)
  
  ## Elatic net
  # Make predictions
  elastic_pred = stats::predict(model_elastic, newx = X[test, ])
  # Evaluate predictions
  metrics.elastic = get_all_6_metrics(test_Y = y[test], test_Y_Hat = elastic_pred)
  
  ## Random forest
  # Make predictions
  rf_pred = stats::predict(rf, newdata = as.matrix(X[test, ] ))
  # Make predictions
  rf_pred = stats::predict(rf, newdata = as.matrix(X[test, ] ))
  # Evaluate predictions
  metrics.rf = get_all_6_metrics(test_Y = y[test], test_Y_Hat = rf_pred)
  
  ## Linear regression
  # Make predictions
  lin_data_X = subset(df_lin, select = -c(y))
  lm_pred = stats::predict(model_linear, newdata = lin_data_X[test, ] )
  # Evaluate predictions
  metrics.linear = get_all_6_metrics(test_Y = y[test], test_Y_Hat = lm_pred)
  
  ## XGboost
  # Make predictions
  xgboost_pred = predict(model_xgb, newdata = as.matrix(X[test, ]))
  # Evaluate predictions
  metrics.xgb = get_all_6_metrics(test_Y = y[test], test_Y_Hat = xgboost_pred)
  
  ## SVM
  # Make predictions
  svm_pred = predict(model_svm, newdata = X[test, ])
  # Evaluate predictions
  metrics.svm = get_all_6_metrics(test_Y = y[test], test_Y_Hat = svm_pred)
  
  ##################################################
  
  # Model Ratios
  
  ## True signal
  ratio.true_signal.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE)  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  ratio.elastic.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE)  
  
  ## Random forest
  ratio.rf.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE) 
  
  ## Linear regression
  ratio.linear.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE)  
  
  ## XGboost
  ratio.xgb.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE)  
  
  ## SVM
  ratio.svm.R2 = get_PR(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE)
  
    ##################################################
  
  # Model Min-Max
  
  ## True signal
  min_max.true_signal.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE)  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  min_max.elastic.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE)  
  
  ## Random forest
  min_max.rf.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE) 
  
  ## Linear regression
  min_max.linear.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE)  
  
  ## XGboost
  min_max.xgb.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE)  
  
  ## SVM
  min_max.svm.R2 = get_min_max(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE)  
  
  ##################################################
  
  # ND-RMSE

  ## True signal
  nd_rmse.true_signal.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.true_signal$MSE,
                              test_Y = y[test])  
  
  ## Null model -> Is used for PR so NA
  
  ## Elatic net
  nd_rmse.elastic.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.elastic$MSE,
                              test_Y = y[test])  
  
  ## Random forest
  nd_rmse.rf.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.rf$MSE,
                             test_Y = y[test]) 
  
  ## Linear regression
  nd_rmse.linear.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.linear$MSE,
                              test_Y = y[test])  
  
  ## XGboost
  nd_rmse.xgb.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.xgb$MSE,
                              test_Y = y[test])  
  
  ## SVM
  nd_rmse.svm.R2 = get_NDRMSE(MSE_null = metrics.null_R2$MSE,
                              MSE_model = metrics.svm$MSE,
                              test_Y = y[test])  

  
  ##################################################
  
  # Summary
  df_summary = data.frame(
    Metric = c(names(metrics.true_signal), "PR", "MinMax", "NDRMSE"),
    True_signal = c(unname(unlist(metrics.true_signal)), ratio.true_signal.R2, min_max.true_signal.R2, nd_rmse.true_signal.R2),
    Dummy_R2 = c(unname(unlist(metrics.null_R2)), NA, NA, NA),
    elastic_net = c(unname(unlist(metrics.elastic)), ratio.elastic.R2, min_max.elastic.R2, nd_rmse.elastic.R2),
    RF = c(unname(unlist(metrics.rf)), ratio.rf.R2, min_max.rf.R2, nd_rmse.rf.R2),
    Linear_reg = c(unname(unlist(metrics.linear)), ratio.linear.R2, min_max.linear.R2, nd_rmse.linear.R2),
    XGB = c(unname(unlist(metrics.xgb)), ratio.xgb.R2, min_max.xgb.R2, nd_rmse.xgb.R2),
    SVM = c(unname(unlist(metrics.svm)), ratio.svm.R2, min_max.svm.R2, nd_rmse.svm.R2)
  )
  
  # Pivot all columns except for the "metric" column into a new column called "model"
  # The values go into the values column
  df_long = df_summary %>%
    pivot_longer(
      cols = -Metric,
      names_to = "model",
      values_to = "value")

  # Pivot each metric name from the metric column into its own column
  df_wide = df_long %>%
    pivot_wider(
      names_from = Metric,
      values_from = value)
  
  df_final = df_wide %>%
    arrange(desc(PR))
  
  ##################################################
  
  # # Memory clean up
  # objects_to_keep = "df_final"
  # all_objects = ls() # List all environments inside this function
  # to_remove = setdiff(all_objects, objects_to_keep)
  # #print(to_remove)
  # rm(list = to_remove) # Remove
  # 
  # # Force garbage collection
  # gc(verbose = F)
  # invisible(gc(full = TRUE, verbose = F))
  
  # Final function output
  return(df_final)
}


# Sanity check
df_Dutch_summary = generate_summary_table(X = df_Dutch$X, y = df_Dutch$y, signal_type = signal_type_global_NL)
df_Dutch_summary

df_German_summary = generate_summary_table(X = df_German$X, y = df_German$y, signal_type = signal_type_global_DE)
df_German_summary
```

Get NSE and LM values

```{r}
get_NSE_and_LM_from_summary(df_Dutch_summary)
get_NSE_and_LM_from_summary(df_German_summary)
```

```{r}
get_NSE_and_LM_from_summary(df_Dutch_summary)$lm
```

```{r}
get_LM(df_Dutch)
```



## Rank correlation

**FUNCTION EXPLANATION**: `get_rank_cor`  

**Args:**  
1. **df_Dutch** (data frame): performance metrics for models evaluated on the Dutch dataset -> Reference.  
2. **df_German** (data frame): performance metrics for models evaluated on the German dataset -> Non-reference.  
3. **exclude_model** (character vector): models to exclude from ranking comparison (default: `c("True_signal", "Dummy_R2")`).  
4. **metrics_list** (character vector): list of metrics to evaluate rank correlation for (default: `c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE")`).  
5. **higher_is_better_list** (named logical vector): for each metric, `TRUE` if higher values indicate better performance, `FALSE` otherwise. Defaults:  
   - NSE = TRUE, RSE = FALSE, RRMSE = FALSE, NRMSE = FALSE,  
     MAPE = FALSE, PR = TRUE, MSE = FALSE, MinMax = TRUE, NDRMSE = TRUE.  

**Returns:**  
- A named numeric vector of Kendall rank correlations between the “true” ranking (based on Dutch MSE) and the proposed rankings from the German dataset, for each metric in `metrics_list`.  

**Main logic:**  
1. Remove excluded models from the Dutch dataset.  
2. Determine the **true ranking** of models using Dutch data, sorting by lowest MSE.  
   - Assign **fixed reference IDs** \(1, 2, \dots, n\) to these models (each ID corresponds to a specific model).  
3. Remove excluded models from the German dataset.  
4. For each metric in `metrics_list`:  
   - Sort German models by the metric, in ascending or descending order depending on `higher_is_better_list`.  
   - Convert the ordered German model names into **IDs** using the Dutch reference mapping.  
   - These IDs represent the **German ranking in the Dutch ID system**.  
   - Compute Kendall rank correlation between the Dutch fixed IDs and the mapped German IDs.  
5. Return a vector of correlations, named by metric.  

**Example of mapping logic:**  
- Dutch ranking: `["ModelA", "ModelB", "ModelC"]` -> ranks: `1, 2, 3`  
- German ranking: `["ModelC", "ModelA", "ModelB"]` -> Mapped German rankings will be `3, 1, 2`  
- Kendall correlation is computed between `1, 2, 3` and `3, 1, 2`. 

```{r}
get_rank_cor = function(df_Dutch, df_German, exclude_model = c("True_signal", "Dummy_R2"),
                        metrics_list = c("NSE", "RSE", "RRMSE", "NRMSE", "MAPE", "PR", "MSE", "MinMax", "NDRMSE"),
                        higher_is_better_list = c(NSE = TRUE, RSE = FALSE, RRMSE = FALSE, NRMSE = FALSE, 
                                                       MAPE = FALSE, PR = TRUE, MSE = FALSE, MinMax = TRUE, NDRMSE = TRUE)) {
  # Step 1: True ranking -> Based on dutch$MSE
  df_Dutch_5_models = df_Dutch %>% filter(! model %in% exclude_model)
  
  true_ranking_names = df_Dutch_5_models[order(df_Dutch_5_models$MSE), ] %>% pull(model)
  true_ranking_idx = 1:length(true_ranking_names)
  
  print("True ranking list")
  print(true_ranking_names)
  
  # Step 2: Proposed ranking
  df_German_5_models = df_German %>% filter(! model %in% exclude_model)
  
  # Initialize results vector
  results = vector("numeric", length = length(metrics_list))
  names(results) = metrics_list
  
  # Compute rank correlation for each metric
  for (metric in metrics_list) {
    proposed_ranking_names = df_German_5_models[order(df_German_5_models[[metric]], decreasing = higher_is_better_list[metric]), ] %>% 
      pull(model)
    
    proposed_ranking_idx = match(true_ranking_names, proposed_ranking_names)
    
    print(paste("Proposed ranking for -> ", metric))
    print(proposed_ranking_names)
    print(proposed_ranking_idx)
    
    # Compute rank correlation
    results[metric] = cor(true_ranking_idx, proposed_ranking_idx, method = "kendall")
  }
  
  return(results)
}

# Linear reg, elastic net, svm, rf, xgb <= Dutch
# RF, xgb, svm, linear reg, elastic net <= German
get_rank_cor(df_Dutch_summary, df_German_summary, metrics_list = c("MSE"), higher_is_better_list = c("MSE"=F))
```

## Plot X4

```{r}
body(true_signal)[[2]][[4]][[2]][[2+5]][[3]]
```


```{r}
label_lines = sapply(1:n_signal_global, function(i) {
  fn_string = body(true_signal)[[2]][[4]][[2]][[2+i]]
  paste0("Signal type ", i, ": ", deparse(fn_string), " with noise prop = ", noise_global_NL)
})

signal_caption = paste(label_lines, collapse = "\n")
```


```{r}
get_lm_nrmse = function(input_df) {
  # Reformat input dataframe
  tmp_df = as.data.frame(input_df$X)
  tmp_df$y = input_df$y
  
  # Fit a linear regression model
  lm_fit = lm(y ~ ., data = tmp_df)
  y_hat = predict(lm_fit)
  
  # Compute RMSE
  mse = mean( (tmp_df$y - y_hat)^2 )
  round(sqrt(mse) / sd(tmp_df$y) , 4) # diff(range(tmp_df$y))
}

# X4 component
get_y_for_x4 = function(x4, signal_type) {
  expr = body(true_signal)[[2]][[4]][[2]][[2+signal_type]][[3]]
  # print(expr)
  eval(expr, envir = list(x4 = x4))
}

# Function to create plot for each dataset
plot_dataset = function(df, country, color, signal_type) {
  # Extract x4
  x4 = df$X[, 4]
  
  # Get x4 range and create evaluation points
  x4_range = range(x4)
  x4_vals = seq(x4_range[1], x4_range[2], length.out = 1000)
  
  # Compute y values and get overall range
  y_pred = sapply(x4_vals, function(x) get_y_for_x4(x, signal_type = signal_type))
  y_range = range(c(df$y, y_pred))
  
  # print(paste("length(y_pred):", length(y_pred)))
  # print(paste("length(x4_vals):", length(x4_vals)))
  
  # Compute linear NRMSE
  lm_nrmse = get_lm_nrmse(df)
  
  # Get LM plot
  lm_fit = lm(df$y ~ x4)
  
  # Predict over x4_vals
  newdata = data.frame(x4 = x4_vals)
  y_hat_lm = predict(lm_fit, newdata = newdata)
  
  # print(paste("length(y_hat_lm):", length(y_hat_lm)))
  
  # Create plot
  plot(df$X[, 4], df$y,
       xlab = "x4", ylab = "y",
       main = paste0(country, " (", lm_nrmse  ," LM NRMSE)"),
       pch = 16, col = color,
       ylim = y_range, cex = 0.5, cex.main = 1.1)
  
  # Add fitted line
  lines(x4_vals, y_pred+25, col = "green", lwd = 2)
  
  # Add LM line
  lines(x4_vals, y_hat_lm, col = "orange", lwd = 2)
}
```


```{r, dpi=300, fig.width=8, fig.height=5}
# Set up side-by-side plotting
par(mfrow = c(2, 3), oma = c(0, 0, 3, 0))
# par(mfrow = c(2, 3), mar = c(4, 4, 3, 1), oma = c(8, 0, 0, 0) )

# New code:
for (signal_type in 1:n_signal_global) {
  # Generate data
  df_tmp = generate_data(use_original_signal_var = F, 
                         noise = noise_global_NL, type = signal_type)
  plot_dataset(df_tmp, paste("Signal type", signal_type), "blue", signal_type = signal_type)
}

# mtext(signal_caption, side = 1, outer = TRUE, line = 6, cex = 0.8)

# legend("topleft", 
#        legend = c("True Signal", "Linear Regression"),
#        col = c("green", "orange"), 
#        lwd = 2,
#        xpd = NA, 
#        inset = c(1.43, 0.6))  

legend("topleft", 
       legend = c("True Signal", "Linear Regression"),
       col = c("green", "orange"), 
       lwd = 2,
       xpd = NA, 
       inset = c(1.43, 0.5),  
       bg = "white")        

mtext("Observable X-Y Relationships with Increasing True Signal Complexity", side = 3, outer = TRUE, line = 0.1, cex = 1, font = 2)

# Reset plotting parameters
par(mfrow = c(1, 1))
```


# ##############

# 3. Main simulation loop

```{r}
library(progress)
```


**CODE EXPLANATION**: Nested simulation over the "entire" noise-signal problem space.

**Purpose:**
This loop generates the raw data needed for the threshold calibration analysis. It systematically evaluates all 45 combinations of noise and signal type against a fixed reference point.

- **Outer loop** (`sim_idx` from 1 to `B_global`): Represents one full Monte Carlo replication of the experiment. This is repeated 50 times to account for sampling variance. In each replication, a new **fixed reference dataset** (`df_Dutch_local`) is generated.

- **Inner loops** (`signal_type` and `local_noise`): Iterate through all `n_signal_global * n_noise_global` (5 x 9 = 45) combinations for the **testing dataset** (`df_German_local`).

**For each of the 2,250 total datasets generated, the loop computes and stores:**
1.  **`lm`**: The curvature score (LM-NRMSE) of the testing dataset.
2.  **`rank_cor`**: The Kendall's Tau rank correlation between the testing dataset's algorithm ranking and the reference dataset's ranking.


```{r}
# Timing the code
start.time = Sys.time()

pb = progress_bar$new(format = "(:spin) [:bar] :percent [Elapsed time: :elapsedfull || Estimated time remaining: :eta]",
                       total = B_global * n_signal_global * n_noise_global,
                       complete = "=",   # Completion bar character
                       incomplete = "-", # Incomplete bar character
                       current = ">",    # Current bar character
                       clear = FALSE,    # If TRUE, clears the bar when finish
                       width = 100)      # Width of the progress bar



# Code ###########################################

lm_rank_sim_df = data.frame()
sim_Dutch_lm = numeric(B_global)

# Simulation loop
for (sim_idx in 1:B_global) {
  # Reference dataset stays fixed
  df_Dutch_local = generate_data(use_original_signal_var = FALSE, noise = noise_global_NL, type = signal_type_global_NL)
  df_Dutch_local_summary = generate_summary_table(X = df_Dutch_local$X, y = df_Dutch_local$y, signal_type = signal_type_global_NL)
  
  sim_Dutch_lm[sim_idx] = get_LM(df_Dutch_local)
  
  
  # Loop over singal type
  for (signal_type in 1:n_signal_global) {
    # Loop over noise proportion
    for (idx in seq_along(noise_prop_list_global)) {
      local_noise = noise_prop_list_global[idx]
      
      # Generate moving data set
      df_German_local = generate_data(use_original_signal_var = F, 
                         noise = local_noise, type = signal_type)
      
      # Generate summary
      df_German_local_summary = generate_summary_table(X = df_German_local$X, y = df_German_local$y, 
                                                 signal_type = signal_type)
      
      
      # Compute LM NRMSE and store values
      new_row = data.frame(lm = get_LM(df_German_local),
                           rank_cor = get_rank_cor(df_Dutch_local_summary, df_German_local_summary, 
                                                   metrics_list = c("MSE"), 
                                                   higher_is_better_list = c("MSE"=F)),
                           signal_type = signal_type,
                           noise = local_noise,
                           simulation = sim_idx)
      
      lm_rank_sim_df = rbind(lm_rank_sim_df, new_row)
      
      # Update progress bar
      pb$tick()
    }
  }
}

# Code ###########################################


end.time = Sys.time()
time.taken = end.time - start.time
time.taken


# Show results
lm_rank_sim_df
```

Save as .Rdata so that my runs don't get lost

```{r}
# Step 1: Create a timestamp
timestamp = format(Sys.time(), "%Y-%m-%d_%H-%M-%S")

# Step 2: Construct filename with key metadata
filename = sprintf(
  "%s_refNoise=%.2f_refSignal=%s_noiseLvls=%d_B=%d_n=%d.Rdata",
  timestamp,
  noise_global_NL,
  signal_type_global_NL,
  n_signal_global,
  B_global,
  n_global / 4
)

# Step 3: Save the dataframe
save(lm_rank_sim_df, file = filename)
```


# ##############

```{r}
# Load dataseet for visualization purposes
load("2025-06-13_19-57-00_refNoise=0.30_refSignal=4_noiseLvls=5_B=50_n=1000.Rdata")
```


```{r}
library(ggplot2)
```


Encode signal type as factor

```{r}
lm_rank_sim_df$signal_type = as.factor(lm_rank_sim_df$signal_type)

str(lm_rank_sim_df$signal_type)
```



```{r}
sim_1 = lm_rank_sim_df %>% filter(simulation == 1)

sim_1

ggplot(sim_1, aes(x = noise, y = lm, colour = signal_type)) +
  geom_line()
```

# 4. Binomial Proportion Test


```{r}
proportion_test_df = lm_rank_sim_df %>% 
  group_by(noise, signal_type) %>% 
  summarise(sample_prop_name = mean(round(rank_cor, 10) >= 0.4))

proportion_test_df
```

```{r}
reshaped_proportion_test_df = proportion_test_df %>%
  pivot_wider(
    names_from = noise,
    values_from = sample_prop_name,
    names_prefix = "noise_"
  )

reshaped_proportion_test_df %>% arrange(desc(signal_type))
```


```{r}
summary_df = lm_rank_sim_df %>% 
  group_by(noise, signal_type) %>% 
  summarise(sample_prop_name = mean(round(rank_cor, 10) >= 0.4),
            mean_lm = mean(lm),
            sd_lm = sd(lm),
            q5_lm = quantile(lm, 0.05),
            q95_lm = quantile(lm, 0.95),
            mean_rank_cor = mean(rank_cor),
            sd_rank_cor = sd(rank_cor),
            q5_rank_cor = quantile(rank_cor, 0.05),
            median_rank_cor = median(rank_cor),
            q95_rank_cor = quantile(rank_cor, 0.95),
            .groups = "drop")

summary_df$noise = round(summary_df$noise, 5) # avoid floating point problems

summary_df
```

# 5. Visualization

## Noise-LM-Signal

```{r}
label_lines = sapply(1:n_signal_global, function(i) {
  fn_string = body(true_signal)[[2]][[4]][[2]][[2+i]]
  paste0(i, ": ", deparse(fn_string))
})
signal_caption = paste(label_lines, collapse = "\n")
```


```{r}
ggplot(summary_df, aes(x = noise, y = mean_lm, color = signal_type, group = signal_type)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymin = mean_lm - sd_lm,
                  ymax = mean_lm + sd_lm),
              width = 0.025) +
  geom_ribbon(aes(ymin = mean_lm - sd_lm,
                  ymax = mean_lm + sd_lm),
              alpha = 0.2, color = NA) +
  scale_x_continuous(limits = c(0.1, 0.9), breaks = seq(0, 1, by = 0.1)) +
  labs(subtitle = paste("Emprical 90% confidence interval over", B_global, "Monte Carlo runs"),
       title = "Curvature score over Noise score",
       x = "Noise score (Var(noise)/Var(Y))", y = "Curvature score (NRMSE of LM)",
       caption = paste("Signal types:", signal_caption)) +
  theme_minimal() +
  guides(color = guide_legend(title = "Signal type"))
```

```{r}
summary_df %>% filter(noise == noise_global_NL) %>% 
  select(noise, mean_lm) %>% 
  group_by(noise) %>% 
  summarize(mean_lm = mean(mean_lm)) 
```

Set the reference point based on the summary dataframe.

```{r}
ref_point_df = summary_df %>% 
  filter(noise == noise_global_NL, signal_type == signal_type_global_NL) %>% 
  select(mean_lm)

ref_point_df$noise = noise_global_NL
```


```{r}
signal_labels = c("1" = "Linear", "2" = "Quadratic", "3" = "Cubed", 
                   "4" = "Exp + low freq", "5" = "Exp + high freq")

# lm_nois_with_ref_plot = ggplot(summary_df, aes(x = noise, y = mean_lm, color = signal_type, group = signal_type)) +
#   geom_line(alpha = 0.3) +
#   geom_point(alpha = 0.3) +
#   geom_errorbar(aes(ymin = q5_lm,
#                   ymax = q95_lm),
#               width = 0.01) +
#   geom_ribbon(aes(ymin = q5_lm,
#                   ymax = q95_lm),
#               alpha = 0.1, color = NA) +
#   scale_x_continuous(limits = c(0.1, 0.9), breaks = seq(0, 1, by = 0.1)) +
#   geom_point(data = ref_point_df, aes(x = noise, y = mean_lm),
#            color = "black", size = 4, inherit.aes = FALSE) +
#   labs(subtitle = paste("Emprical 90% confidence interval over", B_global, "Monte Carlo runs",
#                         "\nBlack dot = Reference dataset"),
#        title = "Curvature score over Noise score",
#        x = "Noise score (Var(noise)/Var(Y))", y = "Curvature score (NRMSE of LM)",
#        caption = paste("Signal types:", signal_caption)) +
#   theme_minimal() +
#   guides(color = guide_legend(title = "Signal Type"))

lm_nois_with_ref_plot = ggplot(summary_df, 
                               aes(x = noise, y = mean_lm, color = factor(signal_type), group = signal_type)) +
  geom_line(alpha = 0.6) +
  geom_point(alpha = 0.6) +
  geom_errorbar(aes(ymin = mean_lm - sd_lm, ymax = mean_lm + sd_lm), width = 0.01) +
  geom_ribbon(aes(ymin = mean_lm - sd_lm, ymax = mean_lm + sd_lm), alpha = 0.1, color = NA) +
  scale_x_continuous(limits = c(0.1, 0.9), breaks = seq(0, 1, by = 0.1)) +
  scale_color_discrete(labels = signal_labels) +
  geom_point(data = ref_point_df, aes(x = noise, y = mean_lm),
           color = "black", size = 4, inherit.aes = FALSE) +
  labs(title = "Curvature Score Across Noise Levels and Signal Types",
       x = "Noise Proportion Var(noise)/Var(Y)", 
       y = "Mean Curvature Score (LM-NRMSE)",
       subtitle = paste("Mean LM-NRMSE ± 1 SD over", B_global, "Monte Carlo simulations")) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(title = "Signal Type", nrow = 1))

lm_nois_with_ref_plot
```

# ####################

# 6. Threshold Calibration


## Helper functions


**FUNCTION EXPLANATION**: `get_LM_noise_corners`

**Args:**
1.  **`reference_LM`** (numeric): The mean curvature score (LM-NRMSE) of the reference dataset.
2.  **`reference_noise`** (numeric): The noise proportion of the reference dataset.
3.  **`claim_box_noise`** (numeric): The desired threshold (`\delta n`) for noise similarity. This defines the half-width of the claim box on the noise axis.
4.  **`claim_box_LM`** (numeric): The desired threshold (`\delata s`) for signal similarity. This defines the half-height of the claim box on the curvature axis.

**Returns:**
- A named list containing the four coordinates of the rectangular decision boundary (aka the "claim box"):
    - `noise_left_corner`: The minimum noise value of the box.
    - `noise_right_corner`: The maximum noise value of the box.
    - `LM_top_corner`: The maximum curvature value of the box.
    - `LM_bottom_corner`: The minimum curvature value of the box.

**Main logic:**
1.  Calculate the left and right boundaries of the claim box by subtracting and adding the `claim_box_noise` threshold to the `reference_noise`.
2.  Calculate the bottom and top boundaries of the claim box by subtracting and adding the `claim_box_LM` threshold to the `reference_LM` score.
3.  Apply boundary checks to ensure the corners do not extend beyond the possible range of values (e.g., noise proportion cannot be less than 0 or greater than 1 in this experiment; curvature score cannot be negative).
4.  Return the final coordinates of the claim box.


```{r}
get_LM_noise_corners = function(reference_LM, reference_noise, 
                                claim_box_noise, claim_box_LM) {
  # Noise left corner
  noise_left_corner = reference_noise - claim_box_noise

  if (noise_left_corner < min(noise_prop_list_global)) {
    noise_left_corner = min(noise_prop_list_global)
  }
  
  # Noise right corner
  noise_right_corner = reference_noise + claim_box_noise

  if (noise_right_corner > max(noise_prop_list_global)) {
    noise_right_corner = max(noise_prop_list_global)
  }
  
  # LM top corner
  LM_top_corner = mean(reference_LM) + claim_box_LM

  if (LM_top_corner > 1) {
    LM_top_corner = 1
  }
  
  # LM bottom corner
  LM_bottom_corner = mean(reference_LM) - claim_box_LM

  if (LM_bottom_corner < 0) {
    LM_bottom_corner = 0
  }
  
  return(list(noise_left_corner = noise_left_corner,
              noise_right_corner = noise_right_corner,
              LM_top_corner = LM_top_corner,
              LM_bottom_corner = LM_bottom_corner))
}

corners = get_LM_noise_corners(ref_point_df$mean_lm, noise_global_NL, 
                                claim_box_noise_global, claim_box_LM_global)
corners
```

##  Binomial proportion test

```{r}
get_similar_dfs = function(df) {
  # df %>% filter(0.8 <= q5_rank_cor)
  df %>% filter(round(sample_prop_name, 10) >= 0.856)
}

get_number_of_similar_dfs = function(df) {
  # df %>% filter(0.4 <= q5_rank_cor) %>% nrow()
  # df %>% filter(0.8 <= median_rank_cor) %>% nrow()
  df %>% filter(round(sample_prop_name, 10) >= 0.856) %>% nrow()
}


all_similar_df = get_similar_dfs(summary_df)
all_similar_df

# get_number_of_similar_dfs(summary_df)
```

```{r}
# We need to count at least 43 for 0.4 (inclusive) and higher
lm_rank_sim_df %>% 
  filter(noise == 0.6, signal_type == 5) %>% 
  select(rank_cor) %>% table()
```

Avoid floating point errors

```{r}
# get_claim_box_df = function(input_summary_df, input_corner) {
#     input_summary_df %>% 
#       filter(input_corner$LM_bottom_corner <= mean_lm,
#          mean_lm <= input_corner$LM_top_corner,
#          input_corner$noise_left_corner <= noise,
#          noise <= input_corner$noise_right_corner)
# }

get_claim_box_df = function(input_summary_df, input_corner) {
  input_summary_df %>%
    filter(
      between(mean_lm, input_corner$LM_bottom_corner - 1e-8, input_corner$LM_top_corner + 1e-8),
      between(noise, input_corner$noise_left_corner - 1e-8, input_corner$noise_right_corner + 1e-8)
    )
}


get_claim_box_df(summary_df, corners)
```

```{r}
get_claim_box_df(summary_df, corners) %>% distinct()
```



```{r}
all_similar_dfs_plot = lm_nois_with_ref_plot +
  geom_point(all_similar_df, mapping = aes(x = noise, y = mean_lm),
             size = 2, color = "blue") +
  labs(subtitle = paste("Mean LM-NRMSE ± 1 SD over", B_global, "Monte Carlo simulations",
                        "\nBlue dots are datasets similar to the reference dataset (black dot)"))
  
  # labs(subtitle = paste("Emprical 90% confidence interval over", B_global, "Monte Carlo runs",
  #                       "\nBlue dots are datasets similar to the reference dataset (black dot) with rank corr >= 0.4"))

all_similar_dfs_plot
```


## Precision & Coverage

```{r}
# # Claim box threshold
# claim_box_noise_global = 0.2
# claim_box_LM_global = 0.05

corners = get_LM_noise_corners(ref_point_df$mean_lm, noise_global_NL, 
                                claim_box_noise_global, claim_box_LM_global)

corners
```

Precision should be called NPV!! 

**FUNCTION EXPLANATION**: `get_precision` (calculates Negative Predictive Value, NPV)

**Args:**
1.  **`input_summary_df`** (data frame): The full summary results from the simulation, containing the ground truth for all 45 datasets.
2.  **`input_corners`** (list): A named list containing the four corner coordinates of the claim box, as generated by `get_LM_noise_corners`.

**Returns:**
- A named list containing:
    - `precision`: The calculated NPV score.
    - `n_correct_inside_claim_box`: The number of truly similar datasets that were correctly identified within the claim box (True Negatives, TN).
    - `n_datasets_in_claim_box`: The total number of datasets predicted as similar by the framework (True Negatives + False Negatives).
    - `n_ALL_datasets`: The total number of datasets in the simulation (45).

**Main logic:**
1.  Identify all datasets from the `input_summary_df` that fall within the boundaries defined by `input_corners`. This is the set of all datasets **predicted as similar** by the framework.
2.  From this subset, count how many are **truly similar** by applying the binomial proportion test criterion (`sample_prop_name >= 0.856`). This gives the number of True Negatives (TN).
3.  Calculate the NPV by dividing the number of correctly identified similar datasets (TN) by the total number of datasets within the claim box (TN + FN).
4.  Return the final NPV score along with the counts used in the calculation.


```{r}
get_precision = function(input_summary_df, input_corners) {
  claim_box_df = get_claim_box_df(input_summary_df, input_corners)
  
  print(claim_box_df)
  
  # "number of correctly claimed datasets" / "total number of datasets inside the claim box
  n_correct_inside_claim_box = get_number_of_similar_dfs(claim_box_df)
  n_datasets_in_claim_box = nrow(claim_box_df)
  
  precision = n_correct_inside_claim_box / n_datasets_in_claim_box
  
  list(precision = precision, n_correct_inside_claim_box = n_correct_inside_claim_box,
       n_datasets_in_claim_box = n_datasets_in_claim_box, n_ALL_datasets = nrow(input_summary_df))
}

# get_precision(summary_df)
```



Coverage is specificty or TNR! 

**FUNCTION EXPLANATION**: `get_coverage` (calculates Specificity, or True Negative Rate)

**Args:**
1.  **`input_summary_df`** (data frame): The full summary results from the simulation, containing the ground truth for all 45 datasets.
2.  **`input_corners`** (list): A named list containing the four corner coordinates of the claim box, as generated by `get_LM_noise_corners`.

**Returns:**
- A named list containing:
    - `coverage`: The calculated Specificity score.
    - `n_correct_inside_claim_box`: The number of truly similar datasets that were correctly identified within the claim box (True Negatives, TN).
    - `n_similar_with_ALL_datasets`: The total number of truly similar datasets across the entire simulation space (True Negatives + False Positives).
    - `n_ALL_datasets`: The total number of datasets in the simulation (45).

**Main logic:**
1.  Identify all datasets from `input_summary_df` that fall within the claim box boundaries and are also **truly similar** (pass the binomial proportion test). This gives the count of True Negatives (TN).
2.  Count the total number of **truly similar** datasets across the *entire* `input_summary_df`, regardless of whether they are inside or outside the claim box. This gives the total number of actual negatives (TN + FP).
3.  Calculate the Specificity by dividing the number of correctly identified similar datasets inside the claim box (TN) by the total number of truly similar datasets that exist in the entire experiment (TN + FP).
4.  Return the final Specificity score along with the counts used in the calculation.

```{r}
get_coverage = function(input_summary_df, input_corners) {
  claim_box_df = get_claim_box_df(input_summary_df, input_corners)
  
  # "number of correctly claimed datasets inside the claim box" / "total number of truly comparable datasets (both inside and outside the claim box)"
  n_correct_inside_claim_box = get_number_of_similar_dfs(claim_box_df)
  n_similar_with_ALL_datasets = get_number_of_similar_dfs(input_summary_df)
  
  coverage = n_correct_inside_claim_box  / n_similar_with_ALL_datasets
  
  list(coverage = coverage, n_correct_inside_claim_box = n_correct_inside_claim_box,
       n_similar_with_ALL_datasets = n_similar_with_ALL_datasets,
       n_ALL_datasets = nrow(input_summary_df))
}

# get_coverage(summary_df)
```

## Thesis Plot

```{r, dpi=300, fig.width=8, fig.height=5}
claim_box_df = get_claim_box_df(summary_df, corners)

all_similar_dfs_plot +
  geom_rect(aes(xmin = corners$noise_left_corner, xmax = corners$noise_right_corner,
                ymin = corners$LM_bottom_corner, ymax = corners$LM_top_corner),
            color = "green",
            fill = NA,
            linewidth = 0.3) +
  geom_point(claim_box_df, mapping = aes(x = noise, y = mean_lm),
             size = 4, color = "green", shape = 4, stroke = 0.75) +
  labs(title = "Performance of the Optimized Framework on Synthetic Data")
```

```{r}
claim_box_df %>%  distinct()
```


```{r}
precision_easy_rule_claim_box = get_precision(summary_df, corners)
precision_easy_rule_claim_box
print("-----------------------")
coverage_easy_rule_claim_box = get_coverage(summary_df, corners)
coverage_easy_rule_claim_box
```



# ####################

# 7. Naive approach

## Min Box

```{r, dpi=300, fig.width=8, fig.height=5}
corners_naive_min_box = get_LM_noise_corners(ref_point_df$mean_lm, noise_global_NL, 
                                0.0, 0.0)

corners_naive_min_box


naive_min_claim_box_df = get_claim_box_df(summary_df, corners_naive_min_box)

all_similar_dfs_plot +
  geom_rect(aes(xmin = corners_naive_min_box$noise_left_corner, 
                xmax = corners_naive_min_box$noise_right_corner,
                ymin = corners_naive_min_box$LM_bottom_corner, 
                ymax = corners_naive_min_box$LM_top_corner),
            color = "green",
            fill = NA,
            alpha = 0.3) +
  geom_point(naive_min_claim_box_df, mapping = aes(x = noise, y = mean_lm),
             size = 4, color = "green", shape = 4, stroke = 2) +
  labs(caption = "Claim box = Green box with green crosses as claims",
       title = "Naive Conservative Baseline")
```
```{r}
precision_naive_MIN_box =  get_precision(summary_df, corners_naive_min_box)
precision_naive_MIN_box
print("-----------------------")
coverage_naive_MIN_box =  get_coverage(summary_df, corners_naive_min_box)
coverage_naive_MIN_box
```




## Max Box

```{r, dpi=300, fig.width=8, fig.height=5}

corners_naive_MAX_box = get_LM_noise_corners(ref_point_df$mean_lm, noise_global_NL, 
                                0.8, 0.8)

corners_naive_MAX_box


naive_MAX_claim_box_df = get_claim_box_df(summary_df, corners_naive_MAX_box)

all_similar_dfs_plot +
  geom_rect(aes(xmin = corners_naive_MAX_box$noise_left_corner, 
                xmax = corners_naive_MAX_box$noise_right_corner,
                ymin = corners_naive_MAX_box$LM_bottom_corner, 
                ymax = corners_naive_MAX_box$LM_top_corner),
            color = "green",
            fill = NA,
            alpha = 0.3) +
  geom_point(naive_MAX_claim_box_df, mapping = aes(x = noise, y = mean_lm),
             size = 4, color = "green", shape = 4) +
  labs(caption = "Claim box = Green box with green crosses as claims",
       title = "Naive Liberal Baseline")
```
```{r}
precision_naive_MAX_box = get_precision(summary_df, corners_naive_MAX_box)
precision_naive_MAX_box
print("-----------------------")
coverage_naive_MAX_box = get_coverage(summary_df, corners_naive_MAX_box)
coverage_naive_MAX_box
```
## Quality check

```{r}
quality_check_df = data.frame(metrics = c("precision", "coverage"),
                              naive_box_combined = c(precision_naive_MAX_box$precision, coverage_naive_MIN_box$coverage),
                              easy_rule = c(precision_easy_rule_claim_box$precision, coverage_easy_rule_claim_box$coverage))

quality_check_df$easy_rule_bigger_than_naive = quality_check_df$easy_rule >= quality_check_df$naive_box


print(quality_check_df)
```

Precision should be called NPV!! 
Coverage = Specificity 


```{r}
thesis_quality_check_df = data.frame(framework = c("Conservative", "Liberal", "Recommended"),
                              delta_N = c(0, 1, claim_box_noise_global),
                              delta_S = c(0, 1, claim_box_LM_global),
                              NPV = c(1, precision_naive_MAX_box$precision, precision_easy_rule_claim_box$precision),
                              Specificity = c(coverage_naive_MIN_box$coverage, 1, coverage_easy_rule_claim_box$coverage))

thesis_quality_check_df$f1_negative = (2*thesis_quality_check_df$NPV * thesis_quality_check_df$Specificity) / (thesis_quality_check_df$NPV + thesis_quality_check_df$Specificity)

thesis_quality_check_df
```



```{r}
if (precision_easy_rule_claim_box$precision >= precision_naive_MAX_box$precision &&
    coverage_easy_rule_claim_box$coverage >= coverage_naive_MIN_box$coverage) {
  print("YES")
}
```


# ####################

# Appendix: Advanced decision rule

```{r}
# claim_box_noise_global = 0.2
# claim_box_LM_global = 0.05
```


## Helper functions

```{r}
get_claim_box_noise = function(noise_ref_input, noise_diff_input) {
  noise_lower = noise_ref_input - noise_diff_input
  noise_upper = noise_ref_input + noise_diff_input
  
  
  if (noise_lower < min(noise_prop_list_global)) {
    noise_lower = min(noise_prop_list_global)
  }
  
  if (noise_upper > max(noise_prop_list_global)) {
    noise_upper = max(noise_prop_list_global)
  }
  
  seq(from = noise_lower, to = noise_upper -0.1, by = 0.1)
}

get_claim_box_noise(noise_ref_input = 0.5,
                    noise_diff_input = claim_box_noise_global)
```


This is where the new logic happens

```{r}
get_corners_per_box = function(lm_ref_input, noise_new_input, noise_ref_input, lm_diff_input) {
  # NEW LOGIC HERE #######################
  lm_new = lm_ref_input + ((noise_new_input - noise_ref_input) * 0.75) #0.75
  
  list(noise_left = noise_new_input, noise_right = noise_new_input + 0.1,
       lm_bottom = lm_new - lm_diff_input,
       lm_top = lm_new + lm_diff_input
       )
}

get_corners_per_box(lm_ref_input = mean(ref_point_df$mean_lm), 
            noise_new_input = 0.3,  
            noise_ref_input = 0.5, 
            lm_diff_input = claim_box_LM_global)
```

Avoid floating point issues

```{r}
# get_claim_box_df_new = function(input_summary_df, input_corner) {
#     input_summary_df %>% 
#       filter(input_corner$lm_bottom <= mean_lm,
#          mean_lm <= input_corner$lm_top,
#          input_corner$noise_left <= noise,
#          noise <= input_corner$noise_right)
# }

get_claim_box_df_new = function(input_summary_df, input_corner) {
  input_summary_df %>%
    filter(
      between(mean_lm, input_corner$lm_bottom - 1e-8, input_corner$lm_top + 1e-8),
      between(noise, input_corner$noise_left - 1e-8, input_corner$noise_right + 1e-8)
    )
}

```


New precision & coverage function

```{r}
get_precision_advanced = function(input_summary_df, claim_box_input) {

  # "number of correctly claimed datasets" / "total number of datasets inside the claim box
  n_correct_inside_claim_box = get_number_of_similar_dfs(claim_box_input)
  n_datasets_in_claim_box = nrow(claim_box_input)
  
  precision = n_correct_inside_claim_box / n_datasets_in_claim_box
  
  list(precision = precision, n_correct_inside_claim_box = n_correct_inside_claim_box,
       n_datasets_in_claim_box = n_datasets_in_claim_box, n_ALL_datasets = nrow(input_summary_df))
}



get_coverage_advanced = function(input_summary_df, claim_box_input) {

  # "number of correctly claimed datasets inside the claim box" / "total number of truly comparable datasets (both inside and outside the claim box)"
  n_correct_inside_claim_box = get_number_of_similar_dfs(claim_box_input)
  n_similar_with_ALL_datasets = get_number_of_similar_dfs(input_summary_df)
  
  coverage = n_correct_inside_claim_box  / n_similar_with_ALL_datasets
  
  list(coverage = coverage, n_correct_inside_claim_box = n_correct_inside_claim_box,
       n_similar_with_ALL_datasets = n_similar_with_ALL_datasets,
       n_ALL_datasets = nrow(input_summary_df))
}
```



## Decision rule

```{r}
noise_boxes = get_claim_box_noise(noise_ref_input = noise_global_NL,
                    noise_diff_input = claim_box_noise_global)

parent_list = vector("list", length(noise_boxes))

for (idx in seq_along(noise_boxes)) {
  noise_level = noise_boxes[idx]
  print(noise_level)
  
  tmp_corner = get_corners_per_box(lm_ref_input = mean(ref_point_df$mean_lm), 
            noise_new_input = noise_level,  
            noise_ref_input = noise_global_NL, 
            lm_diff_input = claim_box_LM_global)
  
  parent_list[[idx]] = tmp_corner
  
  
}

parent_list
```

```{r}
get_claim_box_df_new(summary_df, parent_list[[2]] )
```



```{r}
claim_box_df_advanced = data.frame()

for (idx in 1:length(parent_list)) {
  print(idx)
  
  new_df = get_claim_box_df_new(summary_df, parent_list[[idx]])
  
  claim_box_df_advanced = rbind(claim_box_df_advanced, new_df)
}

claim_box_df_advanced
```
Eliminate duplicates

```{r}
claim_box_df_advanced = claim_box_df_advanced %>% distinct()
claim_box_df_advanced
```


## Visualize

```{r}
advanced_claim_box_plot = all_similar_dfs_plot +
  geom_point(claim_box_df_advanced, mapping = aes(x = noise, y = mean_lm),
             size = 5, color = "green", shape = 4, stroke = 0.7) +
  labs(caption = paste("Claim box = Green box with green crosses as claims",
       "\nBlue dots = Dataset similar to reference \nBlack dot = Reference dataset"),
       title = "Curvature score over Noise score",
       x = "Noise score (Var(noise)/Var(Y))", y = "Curvature score (NRMSE of LM)")

advanced_claim_box_plot
```

```{r}
advanced_claim_box_plot_all_boxes = advanced_claim_box_plot

for (idx in 1:length(parent_list)) {
  corner = parent_list[[idx]]
  
  
  advanced_claim_box_plot_all_boxes = advanced_claim_box_plot_all_boxes +
      geom_rect(xmin = corner$noise_left, xmax = corner$noise_right,
                ymin = corner$lm_bottom, ymax = corner$lm_top,
            color = "green",
            fill = NA,
            alpha = 0.3)
}

print(advanced_claim_box_plot_all_boxes)
```



```{r}
precision_ADVANCED_rule_claim_box = get_precision_advanced(summary_df, claim_box_df_advanced)
precision_ADVANCED_rule_claim_box
print("-----------------------")
coverage_ADVANCED_rule_claim_box = get_coverage_advanced(summary_df, claim_box_df_advanced)
coverage_ADVANCED_rule_claim_box
```

```{r}
quality_check_df$advanced_box = c(precision_ADVANCED_rule_claim_box$precision, 
                                  coverage_ADVANCED_rule_claim_box$coverage)

quality_check_df
```


```{r}
claim_box_df_advanced
claim_box_df_advanced %>% nrow()
```

```{r}
get_similar_dfs(claim_box_df_advanced) %>%
  distinct()
```



